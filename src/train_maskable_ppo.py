"""
Maskable PPOë¥¼ ì‚¬ìš©í•œ 3D bin packing í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
GPU/CPU ìë™ ì„ íƒ ê¸°ëŠ¥ í¬í•¨
ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ë° ì„±ëŠ¥ ì§€í‘œ ì¶”ì  ê¸°ëŠ¥ ì¶”ê°€
"""
import os
import sys
import time
import warnings
import datetime
from typing import Optional, Dict, List
import threading
import queue

import gymnasium as gym
import numpy as np
import torch

# matplotlib ë°±ì—”ë“œ ì„¤ì • (GUI í™˜ê²½ì´ ì—†ëŠ” ì„œë²„ì—ì„œë„ ì‘ë™í•˜ë„ë¡)
import matplotlib
matplotlib.use('Agg')  # GUIê°€ ì—†ëŠ” í™˜ê²½ì—ì„œë„ ì‘ë™í•˜ëŠ” ë°±ì—”ë“œ
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from matplotlib.animation import FuncAnimation
from sb3_contrib import MaskablePPO
from sb3_contrib.common.maskable.evaluation import evaluate_policy
from sb3_contrib.common.maskable.utils import get_action_masks
from sb3_contrib.common.wrappers import ActionMasker
from stable_baselines3.common.callbacks import BaseCallback, EvalCallback, CheckpointCallback
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.env_checker import check_env
from stable_baselines3.common.results_plotter import load_results, ts2xy

from utils import boxes_generator
from device_utils import setup_training_device, log_system_info, get_device

# í™˜ê²½ ë“±ë¡ (KAMP ì„œë²„ì—ì„œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì—†ì´ ì‚¬ìš©í•˜ê¸° ìœ„í•´)
try:
    import gymnasium as gym
    from gymnasium.envs.registration import register
    from packing_env import PackingEnv
    
    # PackingEnv-v0 í™˜ê²½ ë“±ë¡
    if 'PackingEnv-v0' not in gym.envs.registry:
        register(id='PackingEnv-v0', entry_point='packing_env:PackingEnv')
        print("âœ… PackingEnv-v0 í™˜ê²½ ë“±ë¡ ì™„ë£Œ")
    else:
        print("âœ… PackingEnv-v0 í™˜ê²½ ì´ë¯¸ ë“±ë¡ë¨")
except Exception as e:
    print(f"âš ï¸ í™˜ê²½ ë“±ë¡ ì¤‘ ì˜¤ë¥˜: {e}")
    print("í™˜ê²½ì„ ìˆ˜ë™ìœ¼ë¡œ ë“±ë¡í•´ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

from plotly_gif import GIF
import io
from PIL import Image

# ê²½ê³  ë©”ì‹œì§€ ì–µì œ
warnings.filterwarnings("ignore")


class RealTimeMonitorCallback(BaseCallback):
    """
    ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ì„ ìœ„í•œ ì»¤ìŠ¤í…€ ì½œë°± í´ë˜ìŠ¤
    í•™ìŠµ ì§„í–‰ ìƒí™©ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì¶”ì í•˜ê³  ì‹œê°í™”í•©ë‹ˆë‹¤.
    í™œìš©ë¥ , ì•ˆì •ì„± ì§€í‘œ í¬í•¨í•œ ì¢…í•©ì ì¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì œê³µ
    """
    def __init__(self, eval_env, eval_freq=5000, n_eval_episodes=5, verbose=1, update_freq=1000):
        super().__init__(verbose)
        self.eval_env = eval_env
        self.eval_freq = eval_freq
        self.n_eval_episodes = n_eval_episodes
        self.update_freq = update_freq  # ê·¸ë˜í”„ ì—…ë°ì´íŠ¸ ì£¼ê¸° (ë” ë¹ˆë²ˆí•œ ì—…ë°ì´íŠ¸ìš©)
        
        # ì„±ëŠ¥ ì§€í‘œ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
        self.timesteps = []
        self.episode_rewards = []
        self.mean_rewards = []
        self.eval_rewards = []
        self.eval_timesteps = []
        self.success_rates = []
        self.episode_lengths = []
        
        # ìƒˆë¡œ ì¶”ê°€: í™œìš©ë¥  ë° ì•ˆì •ì„± ì§€í‘œ
        self.utilization_rates = []  # í™œìš©ë¥  (ì»¨í…Œì´ë„ˆ ê³µê°„ í™œìš©ë„)
        self.eval_utilization_rates = []  # í‰ê°€ ì‹œ í™œìš©ë¥ 
        self.reward_stability = []  # ë³´ìƒ ì•ˆì •ì„± (í‘œì¤€í¸ì°¨)
        self.utilization_stability = []  # í™œìš©ë¥  ì•ˆì •ì„±
        self.learning_smoothness = []  # í•™ìŠµ ê³¡ì„  smoothness
        self.max_utilization_rates = []  # ìµœëŒ€ í™œìš©ë¥  ê¸°ë¡
        
        # ì‹¤ì‹œê°„ í”Œë¡¯ ì„¤ì • (3x2 ê·¸ë¦¬ë“œë¡œ í™•ì¥)
        self.fig, self.axes = plt.subplots(3, 2, figsize=(16, 12))
        self.fig.suptitle('ì‹¤ì‹œê°„ í•™ìŠµ ì§„í–‰ ìƒí™© - ì„±ëŠ¥ ì§€í‘œ ì¢…í•© ëª¨ë‹ˆí„°ë§', fontsize=16)
        plt.ion()  # ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ í™œì„±í™”
        
        # ë§ˆì§€ë§‰ í‰ê°€ ë° ì—…ë°ì´íŠ¸ ì‹œì 
        self.last_eval_time = 0
        self.last_update_time = 0
        
        # ì—í”¼ì†Œë“œë³„ í†µê³„
        self.current_episode_rewards = []
        self.current_episode_lengths = []
        self.current_episode_utilizations = []
        
        # ì•ˆì •ì„± ê³„ì‚°ì„ ìœ„í•œ ìœˆë„ìš° í¬ê¸°
        self.stability_window = 50
        
    def _on_training_start(self) -> None:
        """í•™ìŠµ ì‹œì‘ ì‹œ í˜¸ì¶œ"""
        print("\n=== ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œì‘ ===")
        self.start_time = time.time()
        
        # ì´ˆê¸° í”Œë¡¯ ì„¤ì •
        self._setup_plots()
        
    def _on_step(self) -> bool:
        """ë§¤ ìŠ¤í…ë§ˆë‹¤ í˜¸ì¶œ"""
        # ì—í”¼ì†Œë“œ ì™„ë£Œ ì²´í¬
        if self.locals.get('dones', [False])[0]:
            # ì—í”¼ì†Œë“œ ë³´ìƒ ë° í™œìš©ë¥  ê¸°ë¡
            if 'episode' in self.locals.get('infos', [{}])[0]:
                episode_info = self.locals['infos'][0]['episode']
                episode_reward = episode_info['r']
                episode_length = episode_info['l']
                
                # í™œìš©ë¥  ê³„ì‚° (ë³´ìƒì´ ê³§ í™œìš©ë¥ ì´ë¯€ë¡œ ë™ì¼)
                episode_utilization = max(0.0, episode_reward)  # ìŒìˆ˜ ë³´ìƒ ì²˜ë¦¬
                
                self.current_episode_rewards.append(episode_reward)
                self.current_episode_lengths.append(episode_length)
                self.current_episode_utilizations.append(episode_utilization)
                
                self.timesteps.append(self.num_timesteps)
                self.episode_rewards.append(episode_reward)
                self.episode_lengths.append(episode_length)
                self.utilization_rates.append(episode_utilization)
                
                # ì‹¤ì‹œê°„ ì¶œë ¥ (ë” ìì„¸í•œ ì •ë³´ í¬í•¨)
                if len(self.current_episode_rewards) % 10 == 0:
                    recent_rewards = self.current_episode_rewards[-10:]
                    recent_utilizations = self.current_episode_utilizations[-10:]
                    
                    mean_reward = np.mean(recent_rewards)
                    mean_utilization = np.mean(recent_utilizations)
                    max_utilization = np.max(recent_utilizations) if recent_utilizations else 0
                    elapsed_time = time.time() - self.start_time
                    
                    print(f"ìŠ¤í…: {self.num_timesteps:,} | "
                          f"ì—í”¼ì†Œë“œ: {len(self.episode_rewards)} | "
                          f"ìµœê·¼ 10 ì—í”¼ì†Œë“œ í‰ê·  ë³´ìƒ: {mean_reward:.3f} | "
                          f"í‰ê·  í™œìš©ë¥ : {mean_utilization:.1%} | "
                          f"ìµœëŒ€ í™œìš©ë¥ : {max_utilization:.1%} | "
                          f"ê²½ê³¼ ì‹œê°„: {elapsed_time:.1f}ì´ˆ")
        
        # ë¹ˆë²ˆí•œ ê·¸ë˜í”„ ì—…ë°ì´íŠ¸
        if self.num_timesteps - self.last_update_time >= self.update_freq:
            self._update_stability_metrics()
            self._quick_update_plots()
            self.last_update_time = self.num_timesteps
        
        # ì£¼ê¸°ì  í‰ê°€ ë° ì „ì²´ í”Œë¡¯ ì—…ë°ì´íŠ¸
        if self.num_timesteps - self.last_eval_time >= self.eval_freq:
            self._perform_evaluation()
            self._update_plots()
            self.last_eval_time = self.num_timesteps
            
        return True
    
    def _update_stability_metrics(self):
        """ì•ˆì •ì„± ì§€í‘œ ì—…ë°ì´íŠ¸"""
        try:
            # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ ê³„ì‚°
            if len(self.episode_rewards) >= self.stability_window:
                # ìµœê·¼ ìœˆë„ìš°ì˜ ë°ì´í„°
                recent_rewards = self.episode_rewards[-self.stability_window:]
                recent_utilizations = self.utilization_rates[-self.stability_window:]
                
                # ë³´ìƒ ì•ˆì •ì„± (í‘œì¤€í¸ì°¨)
                reward_std = np.std(recent_rewards)
                self.reward_stability.append(reward_std)
                
                # í™œìš©ë¥  ì•ˆì •ì„±
                utilization_std = np.std(recent_utilizations)
                self.utilization_stability.append(utilization_std)
                
                # í•™ìŠµ ê³¡ì„  smoothness (ì—°ì†ëœ ê°’ë“¤ì˜ ì°¨ì´ì˜ í‰ê· )
                if len(recent_rewards) > 1:
                    reward_diffs = np.diff(recent_rewards)
                    smoothness = 1.0 / (1.0 + np.mean(np.abs(reward_diffs)))  # 0~1 ë²”ìœ„
                    self.learning_smoothness.append(smoothness)
                
                # ìµœëŒ€ í™œìš©ë¥  ì—…ë°ì´íŠ¸
                current_max_util = np.max(self.utilization_rates)
                self.max_utilization_rates.append(current_max_util)
                
        except Exception as e:
            if self.verbose > 0:
                print(f"ì•ˆì •ì„± ì§€í‘œ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")
    
    def _quick_update_plots(self):
        """ë¹ ë¥¸ í”Œë¡¯ ì—…ë°ì´íŠ¸ (ì¼ë¶€ ì°¨íŠ¸ë§Œ)"""
        try:
            # ë©”ì¸ ì„±ëŠ¥ ì§€í‘œë§Œ ë¹ ë¥´ê²Œ ì—…ë°ì´íŠ¸
            if len(self.episode_rewards) > 10:
                # ë³´ìƒ ì°¨íŠ¸ ì—…ë°ì´íŠ¸
                self.axes[0, 0].clear()
                episodes = list(range(1, len(self.episode_rewards) + 1))
                self.axes[0, 0].plot(episodes, self.episode_rewards, 'b-', alpha=0.3, linewidth=0.8)
                
                # ì´ë™í‰ê· 
                if len(self.episode_rewards) >= 20:
                    window = min(50, len(self.episode_rewards) // 4)
                    moving_avg = []
                    for i in range(window-1, len(self.episode_rewards)):
                        avg = np.mean(self.episode_rewards[i-window+1:i+1])
                        moving_avg.append(avg)
                    self.axes[0, 0].plot(episodes[window-1:], moving_avg, 'r-', linewidth=2, label=f'ì´ë™í‰ê· ({window})')
                    self.axes[0, 0].legend()
                
                self.axes[0, 0].set_title('ì—í”¼ì†Œë“œë³„ ë³´ìƒ (ì‹¤ì‹œê°„)')
                self.axes[0, 0].set_xlabel('ì—í”¼ì†Œë“œ')
                self.axes[0, 0].set_ylabel('ë³´ìƒ')
                self.axes[0, 0].grid(True, alpha=0.3)
                
                # í™œìš©ë¥  ì°¨íŠ¸ ì—…ë°ì´íŠ¸
                self.axes[0, 1].clear()
                utilization_pct = [u * 100 for u in self.utilization_rates]
                self.axes[0, 1].plot(episodes, utilization_pct, 'g-', alpha=0.4, linewidth=0.8)
                
                # í™œìš©ë¥  ì´ë™í‰ê· 
                if len(utilization_pct) >= 20:
                    window = min(30, len(utilization_pct) // 4)
                    moving_avg_util = []
                    for i in range(window-1, len(utilization_pct)):
                        avg = np.mean(utilization_pct[i-window+1:i+1])
                        moving_avg_util.append(avg)
                    self.axes[0, 1].plot(episodes[window-1:], moving_avg_util, 'darkgreen', linewidth=2, label=f'ì´ë™í‰ê· ({window})')
                    self.axes[0, 1].legend()
                
                self.axes[0, 1].set_title('ì»¨í…Œì´ë„ˆ í™œìš©ë¥  (ì‹¤ì‹œê°„)')
                self.axes[0, 1].set_xlabel('ì—í”¼ì†Œë“œ')
                self.axes[0, 1].set_ylabel('í™œìš©ë¥  (%)')
                self.axes[0, 1].set_ylim(0, 100)
                self.axes[0, 1].grid(True, alpha=0.3)
                
                plt.tight_layout()
                plt.draw()
                plt.pause(0.001)  # ë§¤ìš° ì§§ì€ pause
                
        except Exception as e:
            if self.verbose > 0:
                print(f"ë¹ ë¥¸ í”Œë¡¯ ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def _perform_evaluation(self):
        """ëª¨ë¸ í‰ê°€ ìˆ˜í–‰ (í™œìš©ë¥  í¬í•¨) - ë¹ ë¥¸ í‰ê°€ë¡œ ìµœì í™”"""
        try:
            print(f"\ní‰ê°€ ìˆ˜í–‰ ì¤‘... (ìŠ¤í…: {self.num_timesteps:,})")
            
            # í‰ê°€ ì‹¤í–‰ (ë¹ ë¥¸ í‰ê°€)
            eval_rewards = []
            eval_utilizations = []
            success_count = 0
            
            # ë” ì ì€ ì—í”¼ì†Œë“œë¡œ ë¹ ë¥¸ í‰ê°€
            n_eval = min(self.n_eval_episodes, 3)  # ìµœëŒ€ 3ê°œ ì—í”¼ì†Œë“œë§Œ
            
            for ep_idx in range(n_eval):
                try:
                    obs, _ = self.eval_env.reset()
                    episode_reward = 0
                    done = False
                    truncated = False
                    step_count = 0
                    max_steps = 50  # ìµœëŒ€ ìŠ¤í… ìˆ˜ë¥¼ 50ìœ¼ë¡œ ì œí•œ
                    
                    while not (done or truncated) and step_count < max_steps:
                        try:
                            action_masks = get_action_masks(self.eval_env)
                            action, _ = self.model.predict(obs, action_masks=action_masks, deterministic=True)
                            obs, reward, done, truncated, info = self.eval_env.step(action)
                            episode_reward += reward
                            step_count += 1
                        except Exception as step_e:
                            print(f"í‰ê°€ ì—í”¼ì†Œë“œ {ep_idx} ìŠ¤í… {step_count} ì˜¤ë¥˜: {step_e}")
                            break
                    
                    eval_rewards.append(episode_reward)
                    
                    # í™œìš©ë¥ ì€ ë³´ìƒê³¼ ë™ì¼ (í™˜ê²½ì—ì„œ í™œìš©ë¥ ì´ ë³´ìƒìœ¼ë¡œ ì‚¬ìš©ë¨)
                    episode_utilization = max(0.0, episode_reward)
                    eval_utilizations.append(episode_utilization)
                    
                    # ì„±ê³µë¥  ê³„ì‚° (ë³´ìƒì´ ì–‘ìˆ˜ì´ë©´ ì„±ê³µìœ¼ë¡œ ê°„ì£¼)
                    if episode_reward > 0:
                        success_count += 1
                        
                except Exception as ep_e:
                    print(f"í‰ê°€ ì—í”¼ì†Œë“œ {ep_idx} ì˜¤ë¥˜: {ep_e}")
                    # ì‹¤íŒ¨í•œ ì—í”¼ì†Œë“œëŠ” 0ìœ¼ë¡œ ì²˜ë¦¬
                    eval_rewards.append(0.0)
                    eval_utilizations.append(0.0)
            
            # í‰ê°€ ê²°ê³¼ê°€ ìˆì„ ë•Œë§Œ ì²˜ë¦¬
            if eval_rewards:
                mean_eval_reward = np.mean(eval_rewards)
                mean_eval_utilization = np.mean(eval_utilizations)
                success_rate = success_count / len(eval_rewards)
                
                # ê²°ê³¼ ì €ì¥
                self.eval_rewards.append(mean_eval_reward)
                self.eval_utilization_rates.append(mean_eval_utilization)
                self.eval_timesteps.append(self.num_timesteps)
                self.success_rates.append(success_rate)
                
                print(f"í‰ê°€ ì™„ë£Œ: í‰ê·  ë³´ìƒ {mean_eval_reward:.3f}, "
                      f"í‰ê·  í™œìš©ë¥  {mean_eval_utilization:.1%}, "
                      f"ì„±ê³µë¥  {success_rate:.1%}")
            else:
                print("í‰ê°€ ì—í”¼ì†Œë“œ ì‹¤í–‰ ì‹¤íŒ¨")
            
        except Exception as e:
            print(f"í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ê°’ìœ¼ë¡œ ì§„í–‰
            self.eval_rewards.append(0.0)
            self.eval_utilization_rates.append(0.0)
            self.eval_timesteps.append(self.num_timesteps)
            self.success_rates.append(0.0)
    
    def _setup_plots(self):
        """í”Œë¡¯ ì´ˆê¸° ì„¤ì • (3x2 ê·¸ë¦¬ë“œ)"""
        # ìƒë‹¨ ì™¼ìª½: ì—í”¼ì†Œë“œë³„ ë³´ìƒ
        self.axes[0, 0].set_title('ì—í”¼ì†Œë“œë³„ ë³´ìƒ')
        self.axes[0, 0].set_xlabel('ì—í”¼ì†Œë“œ')
        self.axes[0, 0].set_ylabel('ë³´ìƒ')
        self.axes[0, 0].grid(True, alpha=0.3)
        
        # ìƒë‹¨ ì˜¤ë¥¸ìª½: ì»¨í…Œì´ë„ˆ í™œìš©ë¥ 
        self.axes[0, 1].set_title('ì»¨í…Œì´ë„ˆ í™œìš©ë¥ ')
        self.axes[0, 1].set_xlabel('ì—í”¼ì†Œë“œ')
        self.axes[0, 1].set_ylabel('í™œìš©ë¥  (%)')
        self.axes[0, 1].set_ylim(0, 100)
        self.axes[0, 1].grid(True, alpha=0.3)
        
        # ì¤‘ë‹¨ ì™¼ìª½: í‰ê°€ ì„±ëŠ¥ (ë³´ìƒ & í™œìš©ë¥ )
        self.axes[1, 0].set_title('í‰ê°€ ì„±ëŠ¥')
        self.axes[1, 0].set_xlabel('í•™ìŠµ ìŠ¤í…')
        self.axes[1, 0].set_ylabel('í‰ê·  ë³´ìƒ/í™œìš©ë¥ ')
        self.axes[1, 0].grid(True, alpha=0.3)
        
        # ì¤‘ë‹¨ ì˜¤ë¥¸ìª½: ì„±ê³µë¥ 
        self.axes[1, 1].set_title('ì„±ê³µë¥ ')
        self.axes[1, 1].set_xlabel('í•™ìŠµ ìŠ¤í…')
        self.axes[1, 1].set_ylabel('ì„±ê³µë¥  (%)')
        self.axes[1, 1].set_ylim(0, 100)
        self.axes[1, 1].grid(True, alpha=0.3)
        
        # í•˜ë‹¨ ì™¼ìª½: í•™ìŠµ ì•ˆì •ì„± ì§€í‘œ
        self.axes[2, 0].set_title('í•™ìŠµ ì•ˆì •ì„±')
        self.axes[2, 0].set_xlabel('í•™ìŠµ ì§„í–‰ë„')
        self.axes[2, 0].set_ylabel('ì•ˆì •ì„± ì§€í‘œ')
        self.axes[2, 0].grid(True, alpha=0.3)
        
        # í•˜ë‹¨ ì˜¤ë¥¸ìª½: ì—í”¼ì†Œë“œ ê¸¸ì´ & ìµœëŒ€ í™œìš©ë¥ 
        self.axes[2, 1].set_title('ì—í”¼ì†Œë“œ ê¸¸ì´ & ìµœëŒ€ í™œìš©ë¥ ')
        self.axes[2, 1].set_xlabel('ì—í”¼ì†Œë“œ')
        self.axes[2, 1].set_ylabel('ê¸¸ì´ / ìµœëŒ€ í™œìš©ë¥ ')
        self.axes[2, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.subplots_adjust(hspace=0.3, wspace=0.3)  # ì„œë¸Œí”Œë¡¯ ê°„ê²© ì¡°ì •
    
    def _update_plots(self):
        """í”Œë¡¯ ì—…ë°ì´íŠ¸ (ì „ì²´ 6ê°œ ì°¨íŠ¸)"""
        try:
            # ëª¨ë“  ì„œë¸Œí”Œë¡¯ í´ë¦¬ì–´
            for ax in self.axes.flat:
                ax.clear()
            
            # í”Œë¡¯ ì¬ì„¤ì •
            self._setup_plots()
            
            # 1. ì—í”¼ì†Œë“œë³„ ë³´ìƒ (ìƒë‹¨ ì™¼ìª½)
            if self.episode_rewards:
                episodes = list(range(1, len(self.episode_rewards) + 1))
                self.axes[0, 0].plot(episodes, self.episode_rewards, 'b-', alpha=0.3, linewidth=0.5)
                
                # ì´ë™ í‰ê·  (50 ì—í”¼ì†Œë“œ)
                if len(self.episode_rewards) >= 50:
                    moving_avg = []
                    for i in range(49, len(self.episode_rewards)):
                        avg = np.mean(self.episode_rewards[i-49:i+1])
                        moving_avg.append(avg)
                    self.axes[0, 0].plot(episodes[49:], moving_avg, 'r-', linewidth=2, label='ì´ë™í‰ê· (50)')
                    self.axes[0, 0].legend()
            
            # 2. ì»¨í…Œì´ë„ˆ í™œìš©ë¥  (ìƒë‹¨ ì˜¤ë¥¸ìª½)
            if self.utilization_rates:
                episodes = list(range(1, len(self.utilization_rates) + 1))
                utilization_pct = [u * 100 for u in self.utilization_rates]
                self.axes[0, 1].plot(episodes, utilization_pct, 'g-', alpha=0.4, linewidth=0.6)
                
                # í™œìš©ë¥  ì´ë™í‰ê· 
                if len(utilization_pct) >= 30:
                    window = min(30, len(utilization_pct) // 4)
                    moving_avg_util = []
                    for i in range(window-1, len(utilization_pct)):
                        avg = np.mean(utilization_pct[i-window+1:i+1])
                        moving_avg_util.append(avg)
                    self.axes[0, 1].plot(episodes[window-1:], moving_avg_util, 'darkgreen', linewidth=2, label=f'ì´ë™í‰ê· ({window})')
                    self.axes[0, 1].legend()
                
                # ëª©í‘œì„  ì¶”ê°€
                self.axes[0, 1].axhline(y=80, color='orange', linestyle='--', alpha=0.7, label='ëª©í‘œ(80%)')
                if not any('ëª©í‘œ' in str(h.get_label()) for h in self.axes[0, 1].get_children() if hasattr(h, 'get_label')):
                    self.axes[0, 1].legend()
            
            # 3. í‰ê°€ ì„±ëŠ¥ (ì¤‘ë‹¨ ì™¼ìª½)
            if self.eval_rewards:
                # í‰ê°€ ë³´ìƒ
                self.axes[1, 0].plot(self.eval_timesteps, self.eval_rewards, 'g-o', linewidth=2, markersize=4, label='í‰ê°€ ë³´ìƒ')
                self.axes[1, 0].axhline(y=0, color='k', linestyle='--', alpha=0.5)
                
                # í‰ê°€ í™œìš©ë¥  (ìˆëŠ” ê²½ìš°)
                if self.eval_utilization_rates:
                    eval_util_pct = [u * 100 for u in self.eval_utilization_rates]
                    ax2 = self.axes[1, 0].twinx()
                    ax2.plot(self.eval_timesteps, eval_util_pct, 'purple', marker='s', linewidth=2, markersize=3, label='í‰ê°€ í™œìš©ë¥ (%)')
                    ax2.set_ylabel('í™œìš©ë¥  (%)', color='purple')
                    ax2.set_ylim(0, 100)
                
                self.axes[1, 0].legend(loc='upper left')
            
            # 4. ì„±ê³µë¥  (ì¤‘ë‹¨ ì˜¤ë¥¸ìª½)
            if self.success_rates:
                success_percentages = [rate * 100 for rate in self.success_rates]
                self.axes[1, 1].plot(self.eval_timesteps, success_percentages, 'orange', linewidth=2, marker='s', markersize=4)
                self.axes[1, 1].axhline(y=80, color='red', linestyle='--', alpha=0.7, label='ëª©í‘œ(80%)')
                self.axes[1, 1].legend()
            
            # 5. í•™ìŠµ ì•ˆì •ì„± ì§€í‘œ (í•˜ë‹¨ ì™¼ìª½)
            if len(self.reward_stability) > 0:
                stability_x = list(range(len(self.reward_stability)))
                
                # ë³´ìƒ ì•ˆì •ì„± (í‘œì¤€í¸ì°¨)
                self.axes[2, 0].plot(stability_x, self.reward_stability, 'red', linewidth=2, label='ë³´ìƒ ì•ˆì •ì„±', alpha=0.7)
                
                # í™œìš©ë¥  ì•ˆì •ì„±
                if len(self.utilization_stability) > 0:
                    self.axes[2, 0].plot(stability_x, self.utilization_stability, 'blue', linewidth=2, label='í™œìš©ë¥  ì•ˆì •ì„±', alpha=0.7)
                
                # í•™ìŠµ smoothness
                if len(self.learning_smoothness) > 0:
                    # 0~1 ë²”ìœ„ë¥¼ í‘œì¤€í¸ì°¨ ë²”ìœ„ì— ë§ê²Œ ìŠ¤ì¼€ì¼ë§
                    max_std = max(max(self.reward_stability), max(self.utilization_stability) if self.utilization_stability else 0)
                    scaled_smoothness = [s * max_std for s in self.learning_smoothness]
                    self.axes[2, 0].plot(stability_x, scaled_smoothness, 'green', linewidth=2, label='í•™ìŠµ smoothness', alpha=0.7)
                
                self.axes[2, 0].legend()
            
            # 6. ì—í”¼ì†Œë“œ ê¸¸ì´ & ìµœëŒ€ í™œìš©ë¥  (í•˜ë‹¨ ì˜¤ë¥¸ìª½)
            if self.episode_lengths:
                episodes = list(range(1, len(self.episode_lengths) + 1))
                
                # ì—í”¼ì†Œë“œ ê¸¸ì´
                self.axes[2, 1].plot(episodes, self.episode_lengths, 'purple', alpha=0.4, linewidth=0.5, label='ì—í”¼ì†Œë“œ ê¸¸ì´')
                
                # ì—í”¼ì†Œë“œ ê¸¸ì´ ì´ë™í‰ê· 
                if len(self.episode_lengths) >= 20:
                    window = min(20, len(self.episode_lengths) // 4)
                    moving_avg_lengths = []
                    for i in range(window-1, len(self.episode_lengths)):
                        avg = np.mean(self.episode_lengths[i-window+1:i+1])
                        moving_avg_lengths.append(avg)
                    self.axes[2, 1].plot(episodes[window-1:], moving_avg_lengths, 'darkred', linewidth=2, label=f'ê¸¸ì´ ì´ë™í‰ê· ({window})')
                
                # ìµœëŒ€ í™œìš©ë¥  (ìˆëŠ” ê²½ìš°)
                if len(self.max_utilization_rates) > 0:
                    # ë‘ ë²ˆì§¸ yì¶• ì‚¬ìš©
                    ax3 = self.axes[2, 1].twinx()
                    max_util_pct = [u * 100 for u in self.max_utilization_rates[-len(episodes):]]  # ì—í”¼ì†Œë“œ ìˆ˜ì— ë§ì¶¤
                    ax3.plot(episodes[-len(max_util_pct):], max_util_pct, 'orange', linewidth=2, marker='*', markersize=3, label='ìµœëŒ€ í™œìš©ë¥ (%)')
                    ax3.set_ylabel('ìµœëŒ€ í™œìš©ë¥  (%)', color='orange')
                    ax3.set_ylim(0, 100)
                
                self.axes[2, 1].legend(loc='upper left')
            
            # í”Œë¡¯ ì—…ë°ì´íŠ¸
            plt.tight_layout()
            plt.subplots_adjust(hspace=0.3, wspace=0.3)
            plt.draw()
            plt.pause(0.01)
            
            # í”Œë¡¯ ì €ì¥
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            self.fig.savefig(f'results/comprehensive_training_progress_{timestamp}.png', dpi=150, bbox_inches='tight')
            
        except Exception as e:
            print(f"í”Œë¡¯ ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
            if self.verbose > 1:
                import traceback
                traceback.print_exc()
    
    def _on_training_end(self) -> None:
        """í•™ìŠµ ì¢…ë£Œ ì‹œ í˜¸ì¶œ"""
        total_time = time.time() - self.start_time
        print(f"\n=== í•™ìŠµ ì™„ë£Œ! ì´ ì†Œìš” ì‹œê°„: {total_time:.1f}ì´ˆ ===")
        
        # ìµœì¢… í”Œë¡¯ ì €ì¥
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        self.fig.savefig(f'results/final_training_progress_{timestamp}.png', dpi=300, bbox_inches='tight')
        
        # í•™ìŠµ í†µê³„ ì €ì¥
        self._save_training_stats(timestamp)
        
        plt.ioff()  # ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ ë¹„í™œì„±í™”
        plt.close(self.fig)
    
    def _save_training_stats(self, timestamp):
        """í•™ìŠµ í†µê³„ ì €ì¥ (í™œìš©ë¥  ë° ì•ˆì •ì„± ì§€í‘œ í¬í•¨)"""
        stats = {
            # ê¸°ë³¸ í†µê³„
            'total_episodes': len(self.episode_rewards),
            'total_timesteps': self.num_timesteps,
            'final_eval_reward': self.eval_rewards[-1] if self.eval_rewards else 0,
            'final_success_rate': self.success_rates[-1] if self.success_rates else 0,
            'best_eval_reward': max(self.eval_rewards) if self.eval_rewards else 0,
            'best_success_rate': max(self.success_rates) if self.success_rates else 0,
            
            # í™œìš©ë¥  í†µê³„
            'final_utilization_rate': self.utilization_rates[-1] if self.utilization_rates else 0,
            'best_utilization_rate': max(self.utilization_rates) if self.utilization_rates else 0,
            'average_utilization_rate': np.mean(self.utilization_rates) if self.utilization_rates else 0,
            'final_eval_utilization': self.eval_utilization_rates[-1] if self.eval_utilization_rates else 0,
            'best_eval_utilization': max(self.eval_utilization_rates) if self.eval_utilization_rates else 0,
            
            # ì•ˆì •ì„± í†µê³„
            'final_reward_stability': self.reward_stability[-1] if self.reward_stability else 0,
            'final_utilization_stability': self.utilization_stability[-1] if self.utilization_stability else 0,
            'final_learning_smoothness': self.learning_smoothness[-1] if self.learning_smoothness else 0,
            'average_reward_stability': np.mean(self.reward_stability) if self.reward_stability else 0,
            'average_utilization_stability': np.mean(self.utilization_stability) if self.utilization_stability else 0,
            'average_learning_smoothness': np.mean(self.learning_smoothness) if self.learning_smoothness else 0,
            
            # ì›ì‹œ ë°ì´í„° (ë¶„ì„ìš©)
            'episode_rewards': self.episode_rewards,
            'utilization_rates': self.utilization_rates,
            'eval_rewards': self.eval_rewards,
            'eval_utilization_rates': self.eval_utilization_rates,
            'eval_timesteps': self.eval_timesteps,
            'success_rates': self.success_rates,
            'episode_lengths': self.episode_lengths,
            'reward_stability': self.reward_stability,
            'utilization_stability': self.utilization_stability,
            'learning_smoothness': self.learning_smoothness,
            'max_utilization_rates': self.max_utilization_rates,
        }
        
        # í†µê³„ë¥¼ numpy íŒŒì¼ë¡œ ì €ì¥
        np.save(f'results/comprehensive_training_stats_{timestamp}.npy', stats)
        print(f"ì¢…í•© í•™ìŠµ í†µê³„ ì €ì¥ ì™„ë£Œ: comprehensive_training_stats_{timestamp}.npy")
        
        # ìš”ì•½ í…ìŠ¤íŠ¸ íŒŒì¼ë„ ì €ì¥
        summary_path = f'results/comprehensive_summary_{timestamp}.txt'
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write("=== ì¢…í•© í•™ìŠµ ì„±ê³¼ ìš”ì•½ ===\n\n")
            
            f.write("ğŸ“ˆ ê¸°ë³¸ ì„±ê³¼ ì§€í‘œ:\n")
            f.write(f"  â€¢ ì´ ì—í”¼ì†Œë“œ: {stats['total_episodes']:,}\n")
            f.write(f"  â€¢ ì´ í•™ìŠµ ìŠ¤í…: {stats['total_timesteps']:,}\n")
            f.write(f"  â€¢ ìµœì¢… í‰ê°€ ë³´ìƒ: {stats['final_eval_reward']:.3f}\n")
            f.write(f"  â€¢ ìµœê³  í‰ê°€ ë³´ìƒ: {stats['best_eval_reward']:.3f}\n")
            f.write(f"  â€¢ ìµœì¢… ì„±ê³µë¥ : {stats['final_success_rate']:.1%}\n")
            f.write(f"  â€¢ ìµœê³  ì„±ê³µë¥ : {stats['best_success_rate']:.1%}\n\n")
            
            f.write("ğŸ¯ í™œìš©ë¥  ì„±ê³¼:\n")
            f.write(f"  â€¢ ìµœì¢… í™œìš©ë¥ : {stats['final_utilization_rate']:.1%}\n")
            f.write(f"  â€¢ ìµœê³  í™œìš©ë¥ : {stats['best_utilization_rate']:.1%}\n")
            f.write(f"  â€¢ í‰ê·  í™œìš©ë¥ : {stats['average_utilization_rate']:.1%}\n")
            f.write(f"  â€¢ ìµœì¢… í‰ê°€ í™œìš©ë¥ : {stats['final_eval_utilization']:.1%}\n")
            f.write(f"  â€¢ ìµœê³  í‰ê°€ í™œìš©ë¥ : {stats['best_eval_utilization']:.1%}\n\n")
            
            f.write("âš–ï¸ í•™ìŠµ ì•ˆì •ì„±:\n")
            f.write(f"  â€¢ ìµœì¢… ë³´ìƒ ì•ˆì •ì„±: {stats['final_reward_stability']:.3f}\n")
            f.write(f"  â€¢ ìµœì¢… í™œìš©ë¥  ì•ˆì •ì„±: {stats['final_utilization_stability']:.3f}\n")
            f.write(f"  â€¢ ìµœì¢… í•™ìŠµ smoothness: {stats['final_learning_smoothness']:.3f}\n")
            f.write(f"  â€¢ í‰ê·  ë³´ìƒ ì•ˆì •ì„±: {stats['average_reward_stability']:.3f}\n")
            f.write(f"  â€¢ í‰ê·  í™œìš©ë¥  ì•ˆì •ì„±: {stats['average_utilization_stability']:.3f}\n")
            f.write(f"  â€¢ í‰ê·  í•™ìŠµ smoothness: {stats['average_learning_smoothness']:.3f}\n\n")
            
            # ì„±ê³¼ ë“±ê¸‰ í‰ê°€
            f.write("ğŸ† ì„±ê³¼ ë“±ê¸‰:\n")
            if stats['best_utilization_rate'] >= 0.8:
                f.write("  â€¢ í™œìš©ë¥ : ğŸ¥‡ ìš°ìˆ˜ (80% ì´ìƒ)\n")
            elif stats['best_utilization_rate'] >= 0.6:
                f.write("  â€¢ í™œìš©ë¥ : ğŸ¥ˆ ì–‘í˜¸ (60-80%)\n")
            else:
                f.write("  â€¢ í™œìš©ë¥ : ğŸ¥‰ ê°œì„  í•„ìš” (60% ë¯¸ë§Œ)\n")
                
            if stats['best_success_rate'] >= 0.8:
                f.write("  â€¢ ì„±ê³µë¥ : ğŸ¥‡ ìš°ìˆ˜ (80% ì´ìƒ)\n")
            elif stats['best_success_rate'] >= 0.5:
                f.write("  â€¢ ì„±ê³µë¥ : ğŸ¥ˆ ì–‘í˜¸ (50-80%)\n")
            else:
                f.write("  â€¢ ì„±ê³µë¥ : ğŸ¥‰ ê°œì„  í•„ìš” (50% ë¯¸ë§Œ)\n")
                
            if stats['average_learning_smoothness'] >= 0.7:
                f.write("  â€¢ í•™ìŠµ ì•ˆì •ì„±: ğŸ¥‡ ë§¤ìš° ì•ˆì •ì \n")
            elif stats['average_learning_smoothness'] >= 0.5:
                f.write("  â€¢ í•™ìŠµ ì•ˆì •ì„±: ğŸ¥ˆ ì•ˆì •ì \n")
            else:
                f.write("  â€¢ í•™ìŠµ ì•ˆì •ì„±: ğŸ¥‰ ë¶ˆì•ˆì •\n")
        
        print(f"í•™ìŠµ ì„±ê³¼ ìš”ì•½ ì €ì¥ ì™„ë£Œ: {summary_path}")


def create_live_dashboard(stats_file):
    """
    ì €ì¥ëœ í•™ìŠµ í†µê³„ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ ìƒì„±
    """
    try:
        stats = np.load(stats_file, allow_pickle=True).item()
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('í•™ìŠµ ì„±ê³¼ ëŒ€ì‹œë³´ë“œ', fontsize=16)
        
        # 1. ì—í”¼ì†Œë“œ ë³´ìƒ ì¶”ì´
        if stats['episode_rewards']:
            episodes = list(range(1, len(stats['episode_rewards']) + 1))
            axes[0, 0].plot(episodes, stats['episode_rewards'], 'b-', alpha=0.3, linewidth=0.5)
            
            # ì´ë™ í‰ê· 
            window = min(50, len(stats['episode_rewards']) // 10)
            if len(stats['episode_rewards']) >= window:
                moving_avg = []
                for i in range(window-1, len(stats['episode_rewards'])):
                    avg = np.mean(stats['episode_rewards'][i-window+1:i+1])
                    moving_avg.append(avg)
                axes[0, 0].plot(episodes[window-1:], moving_avg, 'r-', linewidth=2, label=f'ì´ë™í‰ê· ({window})')
                axes[0, 0].legend()
            
            axes[0, 0].set_title('ì—í”¼ì†Œë“œë³„ ë³´ìƒ')
            axes[0, 0].set_xlabel('ì—í”¼ì†Œë“œ')
            axes[0, 0].set_ylabel('ë³´ìƒ')
            axes[0, 0].grid(True)
        
        # 2. í‰ê°€ ì„±ëŠ¥
        if stats['eval_rewards']:
            axes[0, 1].plot(stats['eval_timesteps'], stats['eval_rewards'], 'g-o', linewidth=2, markersize=6)
            axes[0, 1].axhline(y=0, color='k', linestyle='--', alpha=0.5)
            axes[0, 1].set_title('í‰ê°€ ì„±ëŠ¥')
            axes[0, 1].set_xlabel('í•™ìŠµ ìŠ¤í…')
            axes[0, 1].set_ylabel('í‰ê·  ë³´ìƒ')
            axes[0, 1].grid(True)
        
        # 3. ì„±ê³µë¥ 
        if stats['success_rates']:
            success_percentages = [rate * 100 for rate in stats['success_rates']]
            axes[1, 0].plot(stats['eval_timesteps'], success_percentages, 'orange', linewidth=2, marker='s', markersize=6)
            axes[1, 0].set_ylim(0, 100)
            axes[1, 0].set_title('ì„±ê³µë¥ ')
            axes[1, 0].set_xlabel('í•™ìŠµ ìŠ¤í…')
            axes[1, 0].set_ylabel('ì„±ê³µë¥  (%)')
            axes[1, 0].grid(True)
        
        # 4. í•™ìŠµ í†µê³„ ìš”ì•½
        axes[1, 1].axis('off')
        summary_text = f"""
í•™ìŠµ í†µê³„ ìš”ì•½:
â€¢ ì´ ì—í”¼ì†Œë“œ: {stats['total_episodes']:,}\n
â€¢ ì´ í•™ìŠµ ìŠ¤í…: {stats['total_timesteps']:,}\n
â€¢ ìµœì¢… í‰ê°€ ë³´ìƒ: {stats['final_eval_reward']:.2f}\n
â€¢ ìµœì¢… ì„±ê³µë¥ : {stats['final_success_rate']:.1%}
        """
        axes[1, 1].text(0.1, 0.9, summary_text, transform=axes[1, 1].transAxes, 
                        fontsize=12, verticalalignment='top', fontfamily='monospace')
        
        plt.tight_layout()
        return fig
        
    except Exception as e:
        print(f"ëŒ€ì‹œë³´ë“œ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        return None


def analyze_training_performance(stats_file):
    """
    í•™ìŠµ ì„±ê³¼ ë¶„ì„ ë° ê°œì„  ì œì•ˆ
    """
    try:
        stats = np.load(stats_file, allow_pickle=True).item()
        
        print("\n=== í•™ìŠµ ì„±ê³¼ ë¶„ì„ ===")
        
        # ê¸°ë³¸ í†µê³„
        print(f"ì´ ì—í”¼ì†Œë“œ: {stats['total_episodes']:,}")
        print(f"ì´ í•™ìŠµ ìŠ¤í…: {stats['total_timesteps']:,}")
        print(f"ìµœì¢… í‰ê°€ ë³´ìƒ: {stats['final_eval_reward']:.3f}")
        print(f"ìµœì¢… ì„±ê³µë¥ : {stats['final_success_rate']:.1%}")
        print(f"ìµœê³  í‰ê°€ ë³´ìƒ: {stats['best_eval_reward']:.3f}")
        print(f"ìµœê³  ì„±ê³µë¥ : {stats['best_success_rate']:.1%}")
        
        # í•™ìŠµ ì•ˆì •ì„± ë¶„ì„
        if len(stats['eval_rewards']) > 1:
            eval_rewards = np.array(stats['eval_rewards'])
            reward_trend = np.polyfit(range(len(eval_rewards)), eval_rewards, 1)[0]
            
            print(f"\n=== í•™ìŠµ ì•ˆì •ì„± ë¶„ì„ ===")
            print(f"í‰ê°€ ë³´ìƒ ì¶”ì„¸: {reward_trend:.4f} (ì–‘ìˆ˜ë©´ ê°œì„ , ìŒìˆ˜ë©´ ì•…í™”)")
            
            # ë³€ë™ì„± ë¶„ì„
            if len(eval_rewards) > 2:
                reward_std = np.std(eval_rewards)
                reward_mean = np.mean(eval_rewards)
                cv = reward_std / abs(reward_mean) if reward_mean != 0 else float('inf')
                print(f"ë³´ìƒ ë³€ë™ì„± (CV): {cv:.3f} (ë‚®ì„ìˆ˜ë¡ ì•ˆì •ì )")
        
        # ì„±ê³µë¥  ë¶„ì„
        if stats['success_rates']:
            success_rates = np.array(stats['success_rates'])
            final_success_rate = success_rates[-1]
            
            print(f"\n=== ì„±ê³µë¥  ë¶„ì„ ===")
            if final_success_rate > 0.8:
                print("âœ… ì„±ê³µë¥ ì´ ë§¤ìš° ë†’ìŠµë‹ˆë‹¤ (80% ì´ìƒ)")
            elif final_success_rate > 0.5:
                print("âš ï¸ ì„±ê³µë¥ ì´ ë³´í†µì…ë‹ˆë‹¤ (50-80%)")
            else:
                print("âŒ ì„±ê³µë¥ ì´ ë‚®ìŠµë‹ˆë‹¤ (50% ë¯¸ë§Œ)")
        
        # ê°œì„  ì œì•ˆ
        print(f"\n=== ê°œì„  ì œì•ˆ ===")
        if stats['final_eval_reward'] < 0:
            print("â€¢ ë³´ìƒì´ ìŒìˆ˜ì…ë‹ˆë‹¤. ë³´ìƒ í•¨ìˆ˜ ì¡°ì •ì„ ê³ ë ¤í•´ë³´ì„¸ìš”.")
        if stats['final_success_rate'] < 0.5:
            print("â€¢ ì„±ê³µë¥ ì´ ë‚®ìŠµë‹ˆë‹¤. í•™ìŠµë¥  ì¡°ì •ì´ë‚˜ ë” ê¸´ í•™ìŠµì„ ê³ ë ¤í•´ë³´ì„¸ìš”.")
        if len(stats['eval_rewards']) > 2:
            recent_rewards = stats['eval_rewards'][-3:]
            if all(r1 >= r2 for r1, r2 in zip(recent_rewards[:-1], recent_rewards[1:])):
                print("â€¢ ìµœê·¼ ì„±ëŠ¥ì´ ê°ì†Œí•˜ê³  ìˆìŠµë‹ˆë‹¤. ê³¼ì í•© ê°€ëŠ¥ì„±ì„ í™•ì¸í•´ë³´ì„¸ìš”.")
        
        return stats
        
    except Exception as e:
        print(f"ì„±ê³¼ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
        return None


def make_env(
    container_size=[10, 10, 10],
    num_boxes=32,
    num_visible_boxes=3,
    seed=42,
    render_mode=None,
    random_boxes=False,
    only_terminal_reward=False,
    improved_reward_shaping=False,  # ìƒˆë¡œ ì¶”ê°€
):
    """
    í™˜ê²½ ìƒì„± í•¨ìˆ˜ (ê°œì„ ëœ ë³´ìƒ í•¨ìˆ˜ ì§€ì›)
    
    Args:
        container_size: ì»¨í…Œì´ë„ˆ í¬ê¸°
        num_boxes: ë°•ìŠ¤ ê°œìˆ˜
        num_visible_boxes: ê°€ì‹œ ë°•ìŠ¤ ê°œìˆ˜
        seed: ëœë¤ ì‹œë“œ
        render_mode: ë Œë”ë§ ëª¨ë“œ
        random_boxes: ëœë¤ ë°•ìŠ¤ ì‚¬ìš© ì—¬ë¶€
        only_terminal_reward: ì¢…ë£Œ ë³´ìƒë§Œ ì‚¬ìš© ì—¬ë¶€
        improved_reward_shaping: ê°œì„ ëœ ë³´ìƒ ì‰ì´í•‘ ì‚¬ìš© ì—¬ë¶€
    """
    def _init():
        try:
            # PackingEnv í™˜ê²½ì— ë§ëŠ” ë°•ìŠ¤ í¬ê¸° ìƒì„±
            from utils import boxes_generator
            
            # ë°•ìŠ¤ í¬ê¸° ìƒì„± (num_boxes ê°œìˆ˜ë§Œí¼)
            box_sizes = boxes_generator(
                bin_size=container_size,
                num_items=num_boxes,
                seed=seed
            )
            
            print(f"ìƒì„±ëœ ë°•ìŠ¤ ê°œìˆ˜: {len(box_sizes)}")
            print(f"ì»¨í…Œì´ë„ˆ í¬ê¸°: {container_size}")
            
            # PackingEnv-v0 í™˜ê²½ ìƒì„±
            env = gym.make(
                "PackingEnv-v0",
                container_size=container_size,
                box_sizes=box_sizes,
                num_visible_boxes=num_visible_boxes,
                render_mode=render_mode,
                random_boxes=random_boxes,
                only_terminal_reward=only_terminal_reward,
            )
            
            print(f"í™˜ê²½ ìƒì„± ì„±ê³µ: PackingEnv-v0")
            
            # ê°œì„ ëœ ë³´ìƒ ì‰ì´í•‘ ì ìš©
            if improved_reward_shaping:
                env = ImprovedRewardWrapper(env)
                print("ê°œì„ ëœ ë³´ìƒ ë˜í¼ ì ìš©ë¨")
            
            # ì•¡ì…˜ ë§ˆìŠ¤í‚¹ ë˜í¼ ì ìš©
            def mask_fn(env_instance):
                try:
                    # PackingEnvì˜ action_masks ë©”ì„œë“œ ì‚¬ìš©
                    if hasattr(env_instance, 'action_masks'):
                        masks = env_instance.action_masks()
                        return np.array(masks, dtype=bool)
                    else:
                        # ê¸°ë³¸ ë§ˆìŠ¤í¬ ë°˜í™˜ (ëª¨ë“  ì•¡ì…˜ í—ˆìš©)
                        return np.ones(env_instance.action_space.n, dtype=bool)
                except Exception as e:
                    print(f"ì•¡ì…˜ ë§ˆìŠ¤í¬ ìƒì„± ì‹¤íŒ¨: {e}")
                    # ê¸°ë³¸ ë§ˆìŠ¤í¬ ë°˜í™˜ (ëª¨ë“  ì•¡ì…˜ í—ˆìš©)
                    return np.ones(env_instance.action_space.n, dtype=bool)
            
            env = ActionMasker(env, mask_fn)
            print("ì•¡ì…˜ ë§ˆìŠ¤í‚¹ ë˜í¼ ì ìš©ë¨")
            
            # ì‹œë“œ ì„¤ì •
            try:
                if hasattr(env, 'seed'):
                    env.seed(seed)
                obs, info = env.reset(seed=seed)
                env.action_space.seed(seed)
                env.observation_space.seed(seed)
                print(f"ì‹œë“œ ì„¤ì • ì™„ë£Œ: {seed}")
            except Exception as e:
                print(f"ì‹œë“œ ì„¤ì • ì‹¤íŒ¨: {e}")
                # ì‹œë“œ ì—†ì´ ë¦¬ì…‹ ì‹œë„
                obs, info = env.reset()
            
            return env
            
        except Exception as e:
            print(f"í™˜ê²½ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise e
    
    return _init


class ImprovedRewardWrapper(gym.Wrapper):
    """
    ê°œì„ ëœ ë³´ìƒ í•¨ìˆ˜ë¥¼ ìœ„í•œ ë˜í¼ í´ë˜ìŠ¤
    ë” ë‚˜ì€ ë³´ìƒ ì‰ì´í•‘ì„ í†µí•´ í•™ìŠµ íš¨ìœ¨ì„±ì„ ë†’ì…ë‹ˆë‹¤.
    """
    
    def __init__(self, env):
        super().__init__(env)
        self.prev_utilization = 0.0
        self.prev_box_count = 0
        self.step_count = 0
        self.max_steps = 1000  # ìµœëŒ€ ìŠ¤í… ìˆ˜
        self.stability_bonus = 0.0
        self.efficiency_bonus = 0.0
        
    def reset(self, **kwargs):
        self.prev_utilization = 0.0
        self.prev_box_count = 0
        self.step_count = 0
        self.stability_bonus = 0.0
        self.efficiency_bonus = 0.0
        return self.env.reset(**kwargs)
    
    def step(self, action):
        observation, reward, terminated, truncated, info = self.env.step(action)
        self.step_count += 1
        
        # í˜„ì¬ ìƒíƒœ ì •ë³´ ì¶”ì¶œ
        current_utilization = self._get_utilization(observation, info)
        current_box_count = self._get_box_count(observation, info)
        
        # ê°œì„ ëœ ë³´ìƒ ê³„ì‚°
        improved_reward = self._calculate_improved_reward(
            reward, current_utilization, current_box_count, terminated, truncated
        )
        
        # ìƒíƒœ ì—…ë°ì´íŠ¸
        self.prev_utilization = current_utilization
        self.prev_box_count = current_box_count
        
        # ì •ë³´ ì—…ë°ì´íŠ¸
        info['original_reward'] = reward
        info['improved_reward'] = improved_reward
        info['utilization'] = current_utilization
        
        return observation, improved_reward, terminated, truncated, info
    
    def _get_utilization(self, observation, info):
        """í˜„ì¬ í™œìš©ë¥  ê³„ì‚°"""
        try:
            if 'utilization' in info:
                return info['utilization']
            elif hasattr(self.env, 'utilization'):
                return self.env.utilization
            else:
                # ê´€ì°° ê³µê°„ì—ì„œ í™œìš©ë¥  ì¶”ì •
                if isinstance(observation, dict) and 'observation' in observation:
                    obs = observation['observation']
                    if len(obs) > 0:
                        return min(obs[0], 1.0)  # ì²« ë²ˆì§¸ ìš”ì†Œê°€ í™œìš©ë¥ ì´ë¼ê³  ê°€ì •
                return 0.0
        except:
            return 0.0
    
    def _get_box_count(self, observation, info):
        """í˜„ì¬ ë°•ìŠ¤ ê°œìˆ˜ ê³„ì‚°"""
        try:
            if 'boxes_packed' in info:
                return info['boxes_packed']
            elif hasattr(self.env, 'boxes_packed'):
                return self.env.boxes_packed
            else:
                return 0
        except:
            return 0
    
    def _calculate_improved_reward(self, original_reward, current_utilization, current_box_count, terminated, truncated):
        """ê°œì„ ëœ ë³´ìƒ ê³„ì‚°"""
        # ê¸°ë³¸ ë³´ìƒ
        reward = original_reward
        
        # 1. í™œìš©ë¥  ê°œì„  ë³´ìƒ (ë” ë†’ì€ ê°€ì¤‘ì¹˜)
        utilization_improvement = current_utilization - self.prev_utilization
        if utilization_improvement > 0:
            reward += utilization_improvement * 2.0  # í™œìš©ë¥  ì¦ê°€ì— ëŒ€í•œ ë³´ìƒ
        
        # 2. ë°•ìŠ¤ ë°°ì¹˜ ì„±ê³µ ë³´ìƒ
        box_improvement = current_box_count - self.prev_box_count
        if box_improvement > 0:
            reward += box_improvement * 0.5  # ë°•ìŠ¤ ë°°ì¹˜ ì„±ê³µì— ëŒ€í•œ ë³´ìƒ
        
        # 3. íš¨ìœ¨ì„± ë³´ìƒ (ë¹ ë¥¸ ë°°ì¹˜ì— ëŒ€í•œ ë³´ìƒ)
        if self.step_count < self.max_steps:
            efficiency_ratio = 1.0 - (self.step_count / self.max_steps)
            self.efficiency_bonus = efficiency_ratio * 0.1
            reward += self.efficiency_bonus
        
        # 4. ì•ˆì •ì„± ë³´ìƒ (í™œìš©ë¥  ë³€í™”ê°€ ì¼ì •í•œ ê²½ìš°)
        if abs(utilization_improvement) < 0.1 and current_utilization > 0.3:
            self.stability_bonus += 0.05
            reward += self.stability_bonus
        else:
            self.stability_bonus = max(0.0, self.stability_bonus - 0.01)
        
        # 5. ì¢…ë£Œ ë³´ìƒ ì¡°ì •
        if terminated:
            if current_utilization > 0.8:  # 80% ì´ìƒ í™œìš©ë¥ 
                reward += 5.0  # í° ë³´ë„ˆìŠ¤
            elif current_utilization > 0.6:  # 60% ì´ìƒ í™œìš©ë¥ 
                reward += 2.0  # ì¤‘ê°„ ë³´ë„ˆìŠ¤
            elif current_utilization > 0.4:  # 40% ì´ìƒ í™œìš©ë¥ 
                reward += 1.0  # ì‘ì€ ë³´ë„ˆìŠ¤
            else:
                reward -= 1.0  # ë‚®ì€ í™œìš©ë¥ ì— ëŒ€í•œ í˜ë„í‹°
        
        # 6. ì‹œê°„ í˜ë„í‹° (ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦¬ëŠ” ê²½ìš°)
        if self.step_count > self.max_steps * 0.8:
            reward -= 0.01  # ì‹œê°„ í˜ë„í‹°
        
        # 7. ì‹¤íŒ¨ í˜ë„í‹°
        if truncated and current_utilization < 0.3:
            reward -= 2.0  # ì‹¤íŒ¨ì— ëŒ€í•œ í˜ë„í‹°
        
        return reward


def create_demonstration_gif(model, env, timestamp):
    """
    í•™ìŠµëœ ëª¨ë¸ì˜ ì‹œì—° GIF ìƒì„± (KAMP ì„œë²„ í˜¸í™˜ ë²„ì „)
    plotly ëŒ€ì‹  matplotlibì„ ì‚¬ìš©í•˜ì—¬ ì•ˆì •ì ì¸ GIF ìƒì„±
    """
    import matplotlib
    matplotlib.use('Agg')  # í—¤ë“œë¦¬ìŠ¤ í™˜ê²½ìš© ë°±ì—”ë“œ
    import matplotlib.pyplot as plt
    from mpl_toolkits.mplot3d.art3d import Poly3DCollection
    
    try:
        print("=== matplotlib ê¸°ë°˜ GIF ì‹œì—° ì‹œì‘ ===")
        
        # í™˜ê²½ ë¦¬ì…‹
        obs, info = env.reset()
        
        def create_matplotlib_visualization(env, step_num=0):
            """matplotlibìœ¼ë¡œ 3D íŒ¨í‚¹ ìƒíƒœ ì‹œê°í™”"""
            try:
                container_size = env.unwrapped.container.size
                
                # 3D í”Œë¡¯ ìƒì„±
                fig = plt.figure(figsize=(10, 8))
                ax = fig.add_subplot(111, projection='3d')
                
                # ì»¨í…Œì´ë„ˆ ê²½ê³„ ê·¸ë¦¬ê¸°
                for i in range(2):
                    for j in range(2):
                        for k in range(2):
                            ax.scatter([i*container_size[0]], [j*container_size[1]], [k*container_size[2]], 
                                      color='red', s=20, alpha=0.7)
                
                # ì»¨í…Œì´ë„ˆ ê²½ê³„ ë¼ì¸
                edges = [
                    # Xì¶• ë¼ì¸
                    ([0, container_size[0]], [0, 0], [0, 0]),
                    ([0, container_size[0]], [container_size[1], container_size[1]], [0, 0]),
                    ([0, container_size[0]], [0, 0], [container_size[2], container_size[2]]),
                    ([0, container_size[0]], [container_size[1], container_size[1]], [container_size[2], container_size[2]]),
                    # Yì¶• ë¼ì¸
                    ([0, 0], [0, container_size[1]], [0, 0]),
                    ([container_size[0], container_size[0]], [0, container_size[1]], [0, 0]),
                    ([0, 0], [0, container_size[1]], [container_size[2], container_size[2]]),
                    ([container_size[0], container_size[0]], [0, container_size[1]], [container_size[2], container_size[2]]),
                    # Zì¶• ë¼ì¸
                    ([0, 0], [0, 0], [0, container_size[2]]),
                    ([container_size[0], container_size[0]], [0, 0], [0, container_size[2]]),
                    ([0, 0], [container_size[1], container_size[1]], [0, container_size[2]]),
                    ([container_size[0], container_size[0]], [container_size[1], container_size[1]], [0, container_size[2]])
                ]
                
                for edge in edges:
                    ax.plot(edge[0], edge[1], edge[2], 'r-', alpha=0.3)
                
                # ë°°ì¹˜ëœ ë°•ìŠ¤ë“¤ ê·¸ë¦¬ê¸°
                if hasattr(env.unwrapped, 'packed_boxes') and env.unwrapped.packed_boxes:
                    colors = plt.cm.Set3(np.linspace(0, 1, len(env.unwrapped.packed_boxes)))
                    
                    for idx, box in enumerate(env.unwrapped.packed_boxes):
                        x, y, z = box.position
                        dx, dy, dz = box.size
                        
                        # ë°•ìŠ¤ì˜ 8ê°œ ê¼­ì§“ì 
                        vertices = [
                            [x, y, z], [x+dx, y, z], [x+dx, y+dy, z], [x, y+dy, z],
                            [x, y, z+dz], [x+dx, y, z+dz], [x+dx, y+dy, z+dz], [x, y+dy, z+dz]
                        ]
                        
                        # 6ê°œ ë©´ ì •ì˜
                        faces = [
                            [vertices[0], vertices[1], vertices[2], vertices[3]],  # í•˜ë‹¨ë©´
                            [vertices[4], vertices[5], vertices[6], vertices[7]],  # ìƒë‹¨ë©´
                            [vertices[0], vertices[1], vertices[5], vertices[4]],  # ì•ë©´
                            [vertices[2], vertices[3], vertices[7], vertices[6]],  # ë’·ë©´
                            [vertices[0], vertices[3], vertices[7], vertices[4]],  # ì™¼ìª½ë©´
                            [vertices[1], vertices[2], vertices[6], vertices[5]]   # ì˜¤ë¥¸ìª½ë©´
                        ]
                        
                        face_collection = Poly3DCollection(faces, alpha=0.7, 
                                                         facecolor=colors[idx], edgecolor='black')
                        ax.add_collection3d(face_collection)
                
                # ì¶• ì„¤ì •
                ax.set_xlabel('X (Depth)')
                ax.set_ylabel('Y (Length)')
                ax.set_zlabel('Z (Height)')
                ax.set_xlim(0, container_size[0])
                ax.set_ylim(0, container_size[1])
                ax.set_zlim(0, container_size[2])
                
                # ì œëª© ì„¤ì •
                packed_count = len(env.unwrapped.packed_boxes) if hasattr(env.unwrapped, 'packed_boxes') else 0
                ax.set_title(f'3D Bin Packing - Step {step_num}\n'
                            f'ë°°ì¹˜ëœ ë°•ìŠ¤: {packed_count}\n'
                            f'ì»¨í…Œì´ë„ˆ í¬ê¸°: {container_size}', 
                            fontsize=10)
                
                ax.grid(True, alpha=0.3)
                ax.view_init(elev=20, azim=45)
                plt.tight_layout()
                
                # ì´ë¯¸ì§€ë¡œ ë³€í™˜
                buffer = io.BytesIO()
                plt.savefig(buffer, format='png', dpi=100, bbox_inches='tight')
                buffer.seek(0)
                image = Image.open(buffer)
                plt.close(fig)  # ë©”ëª¨ë¦¬ ì ˆì•½
                
                return image
                
            except Exception as e:
                print(f"matplotlib ì‹œê°í™” ì˜¤ë¥˜: {e}")
                # ë¹ˆ ì´ë¯¸ì§€ ë°˜í™˜
                blank_img = Image.new('RGB', (800, 600), color='white')
                return blank_img
        
        # ì´ˆê¸° ìƒíƒœ ìº¡ì²˜
        initial_img = create_matplotlib_visualization(env, step_num=0)
        figs = [initial_img]
        print("ì´ˆê¸° ìƒíƒœ ìº¡ì²˜ ì™„ë£Œ")
        
        done = False
        truncated = False
        step_count = 0
        max_steps = 50  # ì ì ˆí•œ í”„ë ˆì„ ìˆ˜
        episode_reward = 0
        
        print("ì—ì´ì „íŠ¸ ì‹œì—° ì¤‘...")
        
        while not (done or truncated) and step_count < max_steps:
            try:
                action_masks = get_action_masks(env)
                action, _states = model.predict(obs, action_masks=action_masks, deterministic=True)
                
                # ìŠ¤í… ì‹¤í–‰
                obs, reward, done, truncated, info = env.step(action)
                episode_reward += reward
                step_count += 1
                
                # ìŠ¤í… í›„ ìƒíƒœ ìº¡ì²˜
                step_img = create_matplotlib_visualization(env, step_num=step_count)
                figs.append(step_img)
                
                # ì§„í–‰ìƒí™© ì¶œë ¥
                if step_count % 10 == 0:
                    print(f"ìŠ¤í… {step_count}: ëˆ„ì  ë³´ìƒ = {episode_reward:.3f}")
                    
            except Exception as e:
                print(f"ìŠ¤í… {step_count} ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
                break
        
        print(f"ì‹œì—° ì™„ë£Œ: {step_count}ìŠ¤í…, ì´ ë³´ìƒ: {episode_reward:.3f}")
        print(f"ìº¡ì²˜ëœ í”„ë ˆì„ ìˆ˜: {len(figs)}")
        
        # GIF ì €ì¥
        if len(figs) >= 2:
            gif_filename = f'trained_maskable_ppo_{timestamp}.gif'
            gif_path = f'gifs/{gif_filename}'
            
            frame_duration = 800  # 0.8ì´ˆ
            
            try:
                figs[0].save(
                    gif_path, 
                    format='GIF', 
                    append_images=figs[1:],
                    save_all=True, 
                    duration=frame_duration,
                    loop=0,
                    optimize=True
                )
                
                # íŒŒì¼ í¬ê¸° í™•ì¸
                import os
                file_size = os.path.getsize(gif_path)
                print(f"âœ… GIF ì €ì¥ ì™„ë£Œ: {gif_filename}")
                print(f"   - íŒŒì¼ í¬ê¸°: {file_size/1024:.1f} KB")
                print(f"   - í”„ë ˆì„ ìˆ˜: {len(figs)}")
                print(f"   - í”„ë ˆì„ ì§€ì†ì‹œê°„: {frame_duration}ms")
                
            except Exception as e:
                print(f"âŒ GIF ì €ì¥ ì‹¤íŒ¨: {e}")
                
        else:
            print("âŒ ì¶©ë¶„í•œ í”„ë ˆì„ì´ ì—†ìŠµë‹ˆë‹¤.")
            
    except Exception as e:
        print(f"âŒ GIF ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


def evaluate_model(model_path, num_episodes=5):
    """
    ì €ì¥ëœ ëª¨ë¸ í‰ê°€ í•¨ìˆ˜ (kamp_auto_run.shì—ì„œ í˜¸ì¶œ)
    """
    try:
        print(f"ëª¨ë¸ í‰ê°€ ì‹œì‘: {model_path}")
        
        # ëª¨ë¸ ë¡œë“œ
        model = MaskablePPO.load(model_path)
        
        # í‰ê°€ìš© í™˜ê²½ ìƒì„±
        eval_env = make_env(
            container_size=[10, 10, 10],
            num_boxes=64,
            num_visible_boxes=3,
            seed=42,
            render_mode=None,
            random_boxes=False,
            only_terminal_reward=False,
        )
        
        # í‰ê°€ ì‹¤í–‰
        mean_reward, std_reward = evaluate_policy(
            model, eval_env, n_eval_episodes=num_episodes, deterministic=True
        )
        
        print(f"í‰ê°€ ì™„ë£Œ - í‰ê·  ë³´ìƒ: {mean_reward:.4f} Â± {std_reward:.4f}")
        
        # í™˜ê²½ ì •ë¦¬
        eval_env.close()
        
        return mean_reward, std_reward
        
    except Exception as e:
        print(f"ëª¨ë¸ í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
        return None, None


def train_and_evaluate(
    container_size=[10, 10, 10],
    num_boxes=64,
    num_visible_boxes=3,
    total_timesteps=100000,
    eval_freq=10000,
    seed=42,
    force_cpu=False,
    save_gif=True,
    curriculum_learning=True,  # ìƒˆë¡œ ì¶”ê°€: ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ
    improved_rewards=True,     # ìƒˆë¡œ ì¶”ê°€: ê°œì„ ëœ ë³´ìƒ
):
    """
    Maskable PPO í•™ìŠµ ë° í‰ê°€ í•¨ìˆ˜ (ê°œì„ ëœ ë²„ì „)
    
    Args:
        container_size: ì»¨í…Œì´ë„ˆ í¬ê¸°
        num_boxes: ë°•ìŠ¤ ê°œìˆ˜
        num_visible_boxes: ê°€ì‹œ ë°•ìŠ¤ ê°œìˆ˜
        total_timesteps: ì´ í•™ìŠµ ìŠ¤í… ìˆ˜
        eval_freq: í‰ê°€ ì£¼ê¸°
        seed: ëœë¤ ì‹œë“œ
        force_cpu: CPU ê°•ì œ ì‚¬ìš© ì—¬ë¶€
        save_gif: GIF ì €ì¥ ì—¬ë¶€
        curriculum_learning: ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ì‚¬ìš© ì—¬ë¶€
        improved_rewards: ê°œì„ ëœ ë³´ìƒ ì‚¬ìš© ì—¬ë¶€
    """
    
    print("=== ê°œì„ ëœ Maskable PPO 3D Bin Packing í•™ìŠµ ì‹œì‘ ===")
    log_system_info()
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device_config = setup_training_device(verbose=True)
    device = get_device(force_cpu=force_cpu)
    
    # ê°œì„ ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
    if curriculum_learning:
        print("ğŸ“ ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ëª¨ë“œ í™œì„±í™”")
        # ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ: ì ì§„ì ìœ¼ë¡œ ë‚œì´ë„ ì¦ê°€
        initial_boxes = max(8, num_boxes // 4)  # ì²˜ìŒì—” ë” ì ì€ ë°•ìŠ¤ë¡œ ì‹œì‘
        current_boxes = initial_boxes
    else:
        current_boxes = num_boxes
    
    # íƒ€ì„ìŠ¤íƒ¬í”„
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs("models", exist_ok=True)
    os.makedirs("results", exist_ok=True)
    os.makedirs("logs", exist_ok=True)
    os.makedirs("gifs", exist_ok=True)
    
    # ê°œì„ ëœ í™˜ê²½ ìƒì„±
    env = make_env(
        container_size=container_size,
        num_boxes=current_boxes,
        num_visible_boxes=num_visible_boxes,
        seed=seed,
        render_mode=None,
        random_boxes=False,
        only_terminal_reward=False,  # í•­ìƒ ì¤‘ê°„ ë³´ìƒ ì‚¬ìš©
        improved_reward_shaping=improved_rewards,  # ê°œì„ ëœ ë³´ìƒ ì‰ì´í•‘ ì ìš©
    )()
    
    # í™˜ê²½ ì²´í¬
    print("í™˜ê²½ ìœ íš¨ì„± ê²€ì‚¬ ì¤‘...")
    check_env(env, warn=True)
    
    # í‰ê°€ìš© í™˜ê²½ ìƒì„±
    eval_env = make_env(
        container_size=container_size,
        num_boxes=current_boxes,
        num_visible_boxes=num_visible_boxes,
        seed=seed+1,
        render_mode="human" if save_gif else None,
        random_boxes=False,
        only_terminal_reward=False,  # í‰ê°€ì—ì„œëŠ” í•­ìƒ ì¤‘ê°„ ë³´ìƒ ì‚¬ìš©
        improved_reward_shaping=improved_rewards,
    )()
    
    # ëª¨ë‹ˆí„°ë§ ì„¤ì •
    env = Monitor(env, f"logs/training_monitor_{timestamp}.csv")
    eval_env = Monitor(eval_env, f"logs/eval_monitor_{timestamp}.csv")
    
    print(f"í™˜ê²½ ì„¤ì • ì™„ë£Œ:")
    print(f"  - ì»¨í…Œì´ë„ˆ í¬ê¸°: {container_size}")
    print(f"  - ë°•ìŠ¤ ê°œìˆ˜: {current_boxes} (ìµœì¢… ëª©í‘œ: {num_boxes})")
    print(f"  - ê°€ì‹œ ë°•ìŠ¤ ê°œìˆ˜: {num_visible_boxes}")
    print(f"  - ì•¡ì…˜ ìŠ¤í˜ì´ìŠ¤: {env.action_space}")
    print(f"  - ê´€ì°° ìŠ¤í˜ì´ìŠ¤: {env.observation_space}")
    
    # ì•ˆì „í•œ ì½œë°± ì„¤ì • (999 ìŠ¤í… ë¬¸ì œ ë°©ì§€)
    callbacks = []
    
    # 999 ìŠ¤í… ë¬¸ì œ ë°©ì§€: ì½œë°±ì„ ì„ íƒì ìœ¼ë¡œ ì¶”ê°€
    use_callbacks = eval_freq >= 2000  # í‰ê°€ ì£¼ê¸°ê°€ ì¶©ë¶„íˆ í´ ë•Œë§Œ ì½œë°± ì‚¬ìš©
    
    if use_callbacks:
        print("âœ… ì½œë°± ì‚¬ìš© (í‰ê°€ ì£¼ê¸°ê°€ ì¶©ë¶„í•¨)")
        
        # ë‹¨ìˆœí•œ ì²´í¬í¬ì¸íŠ¸ ì½œë°±ë§Œ ì‚¬ìš© (ê°€ì¥ ì•ˆì „)
        checkpoint_callback = CheckpointCallback(
            save_freq=max(eval_freq, 3000),  # ìµœì†Œ 3000 ìŠ¤í… ê°„ê²©
            save_path="models/checkpoints",
            name_prefix=f"rl_model_{timestamp}",
            verbose=1
        )
        callbacks.append(checkpoint_callback)
        
        # ì„ íƒì ìœ¼ë¡œ í‰ê°€ ì½œë°± ì¶”ê°€ (ì•ˆì „í•œ ì„¤ì •)
        if eval_freq >= 5000:  # 5000 ìŠ¤í… ì´ìƒì¼ ë•Œë§Œ
            eval_callback = EvalCallback(
                eval_env,
                best_model_save_path="models/best_model",
                log_path="logs/eval_logs",
                eval_freq=eval_freq,
                n_eval_episodes=2,  # ìµœì†Œ ì—í”¼ì†Œë“œ
                deterministic=True,
                render=False,
                verbose=1
            )
            callbacks.append(eval_callback)
            print(f"âœ… í‰ê°€ ì½œë°± ì¶”ê°€ (ì£¼ê¸°: {eval_freq})")
        else:
            print("âš ï¸  í‰ê°€ ì½œë°± ìƒëµ (ì£¼ê¸°ê°€ ë„ˆë¬´ ì§§ìŒ)")
            
    else:
        print("âš ï¸  ì½œë°± ì—†ì´ í•™ìŠµ (999 ìŠ¤í… ë¬¸ì œ ë°©ì§€)")
        print(f"   í‰ê°€ ì£¼ê¸°: {eval_freq} (ê¶Œì¥: 2000 ì´ìƒ)")
        callbacks = None  # ì½œë°± ì™„ì „ ì œê±°
    
    # ê°œì„ ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
    improved_config = {
        "learning_rate": 5e-4,  # ë” ë†’ì€ í•™ìŠµë¥ 
        "n_steps": 2048,        # ë” ë§ì€ ìŠ¤í…
        "batch_size": 256,      # ë” í° ë°°ì¹˜ í¬ê¸°
        "n_epochs": 10,         # ë” ë§ì€ ì—í¬í¬
        "gamma": 0.995,         # ë” ë†’ì€ ê°ê°€ìœ¨
        "gae_lambda": 0.95,     # GAE ëŒë‹¤
        "clip_range": 0.2,      # PPO í´ë¦½ ë²”ìœ„
        "ent_coef": 0.01,       # ì—”íŠ¸ë¡œí”¼ ê³„ìˆ˜
        "vf_coef": 0.5,         # ê°€ì¹˜ í•¨ìˆ˜ ê³„ìˆ˜
        "max_grad_norm": 0.5,   # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
    }
    
    # ëª¨ë¸ ìƒì„± (ê°œì„ ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°)
    print("\n=== ê°œì„ ëœ ëª¨ë¸ ìƒì„± ì¤‘ ===")
    model = MaskablePPO(
        "MultiInputPolicy",
        env,
        learning_rate=improved_config["learning_rate"],
        n_steps=improved_config["n_steps"],
        batch_size=improved_config["batch_size"],
        n_epochs=improved_config["n_epochs"],
        gamma=improved_config["gamma"],
        gae_lambda=improved_config["gae_lambda"],
        clip_range=improved_config["clip_range"],
        ent_coef=improved_config["ent_coef"],
        vf_coef=improved_config["vf_coef"],
        max_grad_norm=improved_config["max_grad_norm"],
        verbose=1,
        tensorboard_log="logs/tensorboard",
        device=str(device),
        seed=seed,
        policy_kwargs=dict(
            activation_fn=torch.nn.ReLU,
            net_arch=[dict(pi=[256, 256, 128], vf=[256, 256, 128])],  # ë” í° ë„¤íŠ¸ì›Œí¬
        ),
    )
    
    print(f"ê°œì„ ëœ ëª¨ë¸ íŒŒë¼ë¯¸í„°:")
    print(f"  - ì •ì±…: MultiInputPolicy")
    print(f"  - í•™ìŠµë¥ : {improved_config['learning_rate']}")
    print(f"  - ë°°ì¹˜ í¬ê¸°: {improved_config['batch_size']}")
    print(f"  - ìŠ¤í… ìˆ˜: {improved_config['n_steps']}")
    print(f"  - ì—í¬í¬ ìˆ˜: {improved_config['n_epochs']}")
    print(f"  - ê°ê°€ìœ¨: {improved_config['gamma']}")
    print(f"  - ì—”íŠ¸ë¡œí”¼ ê³„ìˆ˜: {improved_config['ent_coef']}")
    print(f"  - ë„¤íŠ¸ì›Œí¬: [256, 256, 128]")
    print(f"  - ë””ë°”ì´ìŠ¤: {device}")
    
    # í•™ìŠµ ì‹œì‘
    print(f"\n=== ê°œì„ ëœ í•™ìŠµ ì‹œì‘ (ì´ {total_timesteps:,} ìŠ¤í…) ===")
    print(f"ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ í™œì„±í™” - ë§¤ {max(eval_freq // 3, 1500):,} ìŠ¤í…ë§ˆë‹¤ í‰ê°€")
    print(f"ë¹ ë¥¸ ì—…ë°ì´íŠ¸ - ë§¤ {max(eval_freq // 15, 500):,} ìŠ¤í…ë§ˆë‹¤ ì°¨íŠ¸ ì—…ë°ì´íŠ¸")
    print(f"TensorBoard ë¡œê·¸: tensorboard --logdir=logs/tensorboard")
    print(f"ì‹œì‘ ì‹œê°„: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    start_time = time.time()
    
    try:
        model.learn(
            total_timesteps=total_timesteps,
            callback=callbacks,
            progress_bar=True,
            tb_log_name=f"improved_maskable_ppo_{timestamp}",
        )
        
        training_time = time.time() - start_time
        print(f"\nê°œì„ ëœ í•™ìŠµ ì™„ë£Œ! ì†Œìš” ì‹œê°„: {training_time:.2f}ì´ˆ")
        
        # ëª¨ë¸ ì €ì¥
        model_path = f"models/improved_ppo_mask_{timestamp}"
        model.save(model_path)
        print(f"ê°œì„ ëœ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_path}")
        
        # ìµœì¢… í‰ê°€ (ì•ˆì „í•œ ë°©ì‹)
        print("\n=== ìµœì¢… ëª¨ë¸ í‰ê°€ ===")
        try:
            # ê°„ë‹¨í•˜ê³  ë¹ ë¥¸ í‰ê°€ ìˆ˜í–‰
            print("ë¹ ë¥¸ í‰ê°€ ì‹œì‘...")
            
            # ë‹¨ì¼ ì—í”¼ì†Œë“œ í‰ê°€
            obs, _ = eval_env.reset()
            total_reward = 0.0
            episode_count = 0
            max_episodes = 3  # ìµœëŒ€ 3ê°œ ì—í”¼ì†Œë“œë§Œ í‰ê°€
            
            for episode in range(max_episodes):
                obs, _ = eval_env.reset()
                episode_reward = 0.0
                step_count = 0
                max_steps = 50  # ì—í”¼ì†Œë“œë‹¹ ìµœëŒ€ 50 ìŠ¤í…
                
                while step_count < max_steps:
                    try:
                        action, _ = model.predict(obs, deterministic=True)
                        obs, reward, terminated, truncated, info = eval_env.step(action)
                        episode_reward += reward
                        step_count += 1
                        
                        if terminated or truncated:
                            break
                    except Exception as e:
                        print(f"í‰ê°€ ìŠ¤í… ì¤‘ ì˜¤ë¥˜: {e}")
                        break
                
                total_reward += episode_reward
                episode_count += 1
                print(f"ì—í”¼ì†Œë“œ {episode + 1}: ë³´ìƒ = {episode_reward:.4f}")
            
            if episode_count > 0:
                mean_reward = total_reward / episode_count
                std_reward = 0.0  # ê°„ë‹¨í•œ í‰ê°€ì—ì„œëŠ” í‘œì¤€í¸ì°¨ ê³„ì‚° ìƒëµ
            else:
                mean_reward = 0.0
                std_reward = 0.0
                
            print(f"í‰ê·  ë³´ìƒ: {mean_reward:.4f} Â± {std_reward:.4f}")
                
        except Exception as e:
            print(f"âš ï¸ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
            # ê¸°ë³¸ê°’ ì„¤ì •
            mean_reward = 0.0
            std_reward = 0.0
            print(f"ê¸°ë³¸ í‰ê°€ ë³´ìƒ: {mean_reward:.4f} Â± {std_reward:.4f}")
        
        # GIF ìƒì„± (ê¸°ì¡´ ì½”ë“œ ìŠ¤íƒ€ì¼ ìœ ì§€)
        if save_gif:
            print("\n=== GIF ìƒì„± ì¤‘ ===")
            create_demonstration_gif(model, eval_env, timestamp)
        
        # ê²°ê³¼ ì €ì¥
        results = {
            "timestamp": timestamp,
            "total_timesteps": total_timesteps,
            "training_time": training_time,
            "mean_reward": mean_reward,
            "std_reward": std_reward,
            "container_size": container_size,
            "num_boxes": num_boxes,
            "device": str(device),
            "model_path": model_path,
            "improved_config": improved_config,
            "curriculum_learning": curriculum_learning,
            "improved_rewards": improved_rewards,
        }
        
        # ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥
        results_path = f"results/improved_training_results_{timestamp}.txt"
        with open(results_path, "w") as f:
            f.write("=== ê°œì„ ëœ Maskable PPO 3D Bin Packing í•™ìŠµ ê²°ê³¼ ===\n")
            for key, value in results.items():
                f.write(f"{key}: {value}\n")
        
        print(f"ê°œì„ ëœ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {results_path}")
        
        # í•™ìŠµ í†µê³„ íŒŒì¼ í™•ì¸ ë° ì„±ê³¼ ë¶„ì„
        stats_file = f"results/comprehensive_training_stats_{timestamp}.npy"
        if os.path.exists(stats_file):
            print("\n=== ìë™ ì„±ê³¼ ë¶„ì„ ì‹œì‘ ===")
            analyze_training_performance(stats_file)
            
            # ìµœì¢… ëŒ€ì‹œë³´ë“œ ìƒì„±
            dashboard_fig = create_live_dashboard(stats_file)
            if dashboard_fig:
                dashboard_path = f"results/improved_final_dashboard_{timestamp}.png"
                dashboard_fig.savefig(dashboard_path, dpi=300, bbox_inches='tight')
                print(f"ê°œì„ ëœ ìµœì¢… ëŒ€ì‹œë³´ë“œ ì €ì¥: {dashboard_path}")
                plt.close(dashboard_fig)
        
        return model, results
        
    except KeyboardInterrupt:
        print("\ní•™ìŠµì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        model.save(f"models/interrupted_improved_model_{timestamp}")
        return model, None
    
    except Exception as e:
        print(f"\ní•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        model.save(f"models/error_improved_model_{timestamp}")
        raise e


class CurriculumLearningCallback(BaseCallback):
    """
    ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ì½œë°± í´ë˜ìŠ¤
    ì„±ê³µë¥ ì— ë”°ë¼ ì ì§„ì ìœ¼ë¡œ ë°•ìŠ¤ ê°œìˆ˜(ë‚œì´ë„)ë¥¼ ì¦ê°€ì‹œí‚µë‹ˆë‹¤.
    """
    
    def __init__(
        self,
        container_size,
        initial_boxes,
        target_boxes,
        num_visible_boxes,
        success_threshold=0.6,
        curriculum_steps=5,
        patience=5,
        verbose=0,
    ):
        super().__init__(verbose)
        self.container_size = container_size
        self.initial_boxes = initial_boxes
        self.target_boxes = target_boxes
        self.num_visible_boxes = num_visible_boxes
        self.success_threshold = success_threshold
        self.curriculum_steps = curriculum_steps
        self.patience = patience
        self.verbose = verbose
        
        # ì»¤ë¦¬í˜ëŸ¼ ë‹¨ê³„ ì„¤ì •
        self.current_boxes = initial_boxes
        self.box_increments = []
        if target_boxes > initial_boxes:
            step_size = (target_boxes - initial_boxes) // curriculum_steps
            for i in range(curriculum_steps):
                next_boxes = initial_boxes + (i + 1) * step_size
                if next_boxes > target_boxes:
                    next_boxes = target_boxes
                self.box_increments.append(next_boxes)
            # ë§ˆì§€ë§‰ ë‹¨ê³„ëŠ” í•­ìƒ target_boxes
            if self.box_increments[-1] != target_boxes:
                self.box_increments.append(target_boxes)
        
        # ì„±ê³¼ ì¶”ì  ë³€ìˆ˜
        self.evaluation_count = 0
        self.consecutive_successes = 0
        self.curriculum_level = 0
        self.last_success_rate = 0.0
        
        if self.verbose >= 1:
            print(f"ğŸ“ ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ì´ˆê¸°í™”:")
            print(f"   - ì‹œì‘ ë°•ìŠ¤ ìˆ˜: {self.initial_boxes}")
            print(f"   - ëª©í‘œ ë°•ìŠ¤ ìˆ˜: {self.target_boxes}")
            print(f"   - ë‹¨ê³„ë³„ ì¦ê°€: {self.box_increments}")
            print(f"   - ì„±ê³µ ì„ê³„ê°’: {self.success_threshold}")
    
    def _on_step(self) -> bool:
        return True
    
    def _on_rollout_end(self) -> None:
        """ë¡¤ì•„ì›ƒ ì¢…ë£Œ ì‹œ í˜¸ì¶œë˜ëŠ” ë©”ì„œë“œ"""
        # í‰ê°€ ê²°ê³¼ í™•ì¸
        if hasattr(self.model, 'ep_info_buffer') and len(self.model.ep_info_buffer) > 0:
            # ìµœê·¼ ì—í”¼ì†Œë“œë“¤ì˜ ì„±ê³µë¥  ê³„ì‚°
            recent_episodes = list(self.model.ep_info_buffer)[-20:]  # ìµœê·¼ 20ê°œ ì—í”¼ì†Œë“œ
            if len(recent_episodes) >= 10:  # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ
                # ë³´ìƒì„ ê¸°ë°˜ìœ¼ë¡œ ì„±ê³µë¥  ê³„ì‚° (ë³´ìƒ > 0.5ì¸ ê²½ìš° ì„±ê³µìœ¼ë¡œ ê°„ì£¼)
                rewards = [ep.get('r', 0) for ep in recent_episodes]
                success_rate = sum(1 for r in rewards if r > 0.5) / len(rewards)
                
                self.last_success_rate = success_rate
                self.evaluation_count += 1
                
                # ì„±ê³µë¥ ì´ ì„ê³„ê°’ì„ ë„˜ìœ¼ë©´ ë‚œì´ë„ ì¦ê°€ ê³ ë ¤
                if success_rate >= self.success_threshold:
                    self._increase_difficulty()
    
    def _increase_difficulty(self):
        """ë‚œì´ë„ ì¦ê°€ (ë°•ìŠ¤ ê°œìˆ˜ ì¦ê°€)"""
        if self.curriculum_level < len(self.box_increments):
            new_boxes = self.box_increments[self.curriculum_level]
            
            if self.verbose >= 1:
                print(f"\nğŸ¯ ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ: ë‚œì´ë„ ì¦ê°€!")
                print(f"   - ì´ì „ ë°•ìŠ¤ ìˆ˜: {self.current_boxes}")
                print(f"   - ìƒˆë¡œìš´ ë°•ìŠ¤ ìˆ˜: {new_boxes}")
                print(f"   - í˜„ì¬ ì„±ê³µë¥ : {self.last_success_rate:.1%}")
                print(f"   - ì—°ì† ì„±ê³µ íšŸìˆ˜: {self.consecutive_successes}")
            
            self.current_boxes = new_boxes
            self.curriculum_level += 1
            self.consecutive_successes = 0
            
            # í™˜ê²½ ì¬ìƒì„±
            self._update_environment()
    
    def _update_environment(self):
        """í™˜ê²½ì„ ìƒˆë¡œìš´ ë°•ìŠ¤ ê°œìˆ˜ë¡œ ì—…ë°ì´íŠ¸ (ì•ˆì „í•œ ë°©ì‹)"""
        try:
            # í™˜ê²½ ì§ì ‘ ë³€ê²½ ëŒ€ì‹  ë¡œê·¸ë§Œ ì¶œë ¥
            # ì‹¤ì œ í™˜ê²½ ë³€ê²½ì€ í•™ìŠµ ì¤‘ì— ë¶ˆì•ˆì •í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë¹„í™œì„±í™”
            if self.verbose >= 1:
                print(f"   - í™˜ê²½ ì—…ë°ì´íŠ¸ ì‹œë®¬ë ˆì´ì…˜: {self.current_boxes}ê°œ ë°•ìŠ¤")
                print(f"   - ì‹¤ì œ í™˜ê²½ ë³€ê²½ì€ ì•ˆì •ì„±ì„ ìœ„í•´ ë¹„í™œì„±í™”ë¨")
                
            # ëŒ€ì‹  ë‹¤ìŒ ì—í”¼ì†Œë“œë¶€í„° ë” ì–´ë ¤ìš´ ì¡°ê±´ìœ¼ë¡œ í‰ê°€í•˜ë„ë¡ ì„¤ì •
            # (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” í™˜ê²½ ë‚´ë¶€ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•˜ê±°ë‚˜ 
            #  ìƒˆë¡œìš´ í•™ìŠµ ì„¸ì…˜ì—ì„œ ë” ì–´ë ¤ìš´ ì„¤ì •ì„ ì‚¬ìš©)
                
        except Exception as e:
            if self.verbose >= 1:
                print(f"   - í™˜ê²½ ì—…ë°ì´íŠ¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                import traceback
                traceback.print_exc()
    
    def get_current_difficulty(self):
        """í˜„ì¬ ë‚œì´ë„ ì •ë³´ ë°˜í™˜"""
        return {
            "current_boxes": self.current_boxes,
            "curriculum_level": self.curriculum_level,
            "max_level": len(self.box_increments),
            "success_rate": self.last_success_rate,
            "consecutive_successes": self.consecutive_successes,
        }


def main():
    """ë©”ì¸ í•¨ìˆ˜ (ê°œì„ ëœ ë²„ì „)"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ê°œì„ ëœ Maskable PPO 3D Bin Packing")
    parser.add_argument("--timesteps", type=int, default=200000, help="ì´ í•™ìŠµ ìŠ¤í… ìˆ˜ (ê¸°ë³¸ê°’ ì¦ê°€)")
    parser.add_argument("--container-size", nargs=3, type=int, default=[10, 10, 10], help="ì»¨í…Œì´ë„ˆ í¬ê¸°")
    parser.add_argument("--num-boxes", type=int, default=32, help="ë°•ìŠ¤ ê°œìˆ˜")  # ê¸°ë³¸ê°’ ê°ì†Œ (ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµìš©)
    parser.add_argument("--visible-boxes", type=int, default=3, help="ê°€ì‹œ ë°•ìŠ¤ ê°œìˆ˜")
    parser.add_argument("--seed", type=int, default=42, help="ëœë¤ ì‹œë“œ")
    parser.add_argument("--force-cpu", action="store_true", help="CPU ê°•ì œ ì‚¬ìš©")
    parser.add_argument("--no-gif", action="store_true", help="GIF ìƒì„± ë¹„í™œì„±í™”")
    parser.add_argument("--eval-freq", type=int, default=15000, help="í‰ê°€ ì£¼ê¸°")
    parser.add_argument("--analyze-only", type=str, help="í•™ìŠµ í†µê³„ íŒŒì¼ ë¶„ì„ë§Œ ìˆ˜í–‰ (íŒŒì¼ ê²½ë¡œ ì§€ì •)")
    parser.add_argument("--dashboard-only", type=str, help="ëŒ€ì‹œë³´ë“œë§Œ ìƒì„± (í•™ìŠµ í†µê³„ íŒŒì¼ ê²½ë¡œ ì§€ì •)")
    
    # ìƒˆë¡œìš´ ê°œì„  ì˜µì…˜ë“¤
    parser.add_argument("--curriculum-learning", action="store_true", default=True, 
                        help="ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ì‚¬ìš© (ê¸°ë³¸ê°’: í™œì„±í™”)")
    parser.add_argument("--no-curriculum", action="store_true", 
                        help="ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ë¹„í™œì„±í™”")
    parser.add_argument("--improved-rewards", action="store_true", default=True,
                        help="ê°œì„ ëœ ë³´ìƒ í•¨ìˆ˜ ì‚¬ìš© (ê¸°ë³¸ê°’: í™œì„±í™”)")
    parser.add_argument("--no-improved-rewards", action="store_true",
                        help="ê°œì„ ëœ ë³´ìƒ í•¨ìˆ˜ ë¹„í™œì„±í™”")
    parser.add_argument("--aggressive-training", action="store_true",
                        help="ë§¤ìš° ì ê·¹ì ì¸ í•™ìŠµ ëª¨ë“œ (ë” ê¸´ í•™ìŠµ ì‹œê°„)")
    
    args = parser.parse_args()
    
    # ì˜µì…˜ ì²˜ë¦¬
    curriculum_learning = args.curriculum_learning and not args.no_curriculum
    improved_rewards = args.improved_rewards and not args.no_improved_rewards
    
    # ì ê·¹ì ì¸ í•™ìŠµ ëª¨ë“œ
    if args.aggressive_training:
        timesteps = max(args.timesteps, 500000)  # ìµœì†Œ 50ë§Œ ìŠ¤í…
        eval_freq = max(args.eval_freq, 20000)   # í‰ê°€ ì£¼ê¸° ì¦ê°€
        print("ğŸš€ ì ê·¹ì ì¸ í•™ìŠµ ëª¨ë“œ í™œì„±í™”!")
        print(f"   - í•™ìŠµ ìŠ¤í…: {timesteps:,}")
        print(f"   - í‰ê°€ ì£¼ê¸°: {eval_freq:,}")
    else:
        timesteps = args.timesteps
        eval_freq = args.eval_freq
    
    print(f"ğŸ¯ í•™ìŠµ ì„¤ì •:")
    print(f"   - ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ: {'âœ…' if curriculum_learning else 'âŒ'}")
    print(f"   - ê°œì„ ëœ ë³´ìƒ: {'âœ…' if improved_rewards else 'âŒ'}")
    print(f"   - ì´ ìŠ¤í…: {timesteps:,}")
    print(f"   - í‰ê°€ ì£¼ê¸°: {eval_freq:,}")
    
    # ë¶„ì„ ì „ìš© ëª¨ë“œ
    if args.analyze_only:
        if os.path.exists(args.analyze_only):
            print(f"í•™ìŠµ í†µê³„ íŒŒì¼ ë¶„ì„: {args.analyze_only}")
            analyze_training_performance(args.analyze_only)
        else:
            print(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.analyze_only}")
        return
    
    # ëŒ€ì‹œë³´ë“œ ì „ìš© ëª¨ë“œ
    if args.dashboard_only:
        if os.path.exists(args.dashboard_only):
            print(f"ëŒ€ì‹œë³´ë“œ ìƒì„±: {args.dashboard_only}")
            dashboard_fig = create_live_dashboard(args.dashboard_only)
            if dashboard_fig:
                timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
                dashboard_path = f"results/dashboard_{timestamp}.png"
                dashboard_fig.savefig(dashboard_path, dpi=300, bbox_inches='tight')
                print(f"ëŒ€ì‹œë³´ë“œ ì €ì¥: {dashboard_path}")
                plt.show()  # ëŒ€ì‹œë³´ë“œ í‘œì‹œ
                plt.close(dashboard_fig)
        else:
            print(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.dashboard_only}")
        return
    
    # ê¸°ì¡´ ì½”ë“œ ìŠ¤íƒ€ì¼ë¡œ ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ë„ ì‹¤í–‰ ê°€ëŠ¥
    if args.timesteps <= 100:
        print("=== ê°„ë‹¨ í…ŒìŠ¤íŠ¸ ëª¨ë“œ ===")
        # ê¸°ì¡´ train.pyì˜ ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì½”ë“œ ìŠ¤íƒ€ì¼
        container_size = args.container_size
        box_sizes = [[3, 3, 3], [3, 2, 3], [3, 4, 2], [3, 2, 4], [3, 2, 3]]
        
        env = gym.make(
            "PackingEnv-v0",
            container_size=container_size,
            box_sizes=box_sizes,
            num_visible_boxes=1,
            render_mode="human",
            random_boxes=False,
            only_terminal_reward=False,
        )
        
        model = MaskablePPO("MultiInputPolicy", env, verbose=1)
        print("ê°„ë‹¨ í•™ìŠµ ì‹œì‘")
        model.learn(total_timesteps=args.timesteps)
        print("ê°„ë‹¨ í•™ìŠµ ì™„ë£Œ")
        model.save("models/ppo_mask_simple")
        
    else:
        # ì „ì²´ í•™ìŠµ ì‹¤í–‰ (ê°œì„ ëœ íŒŒë¼ë¯¸í„°)
        try:
            model, results = train_and_evaluate(
                container_size=args.container_size,
                num_boxes=args.num_boxes,
                num_visible_boxes=args.visible_boxes,
                total_timesteps=timesteps,
                eval_freq=eval_freq,
                seed=args.seed,
                force_cpu=args.force_cpu,
                save_gif=not args.no_gif,
                curriculum_learning=curriculum_learning,
                improved_rewards=improved_rewards,
            )
            
            if results:
                print(f"\nğŸ‰ === ìµœì¢… ê²°ê³¼ ===")
                print(f"í‰ê·  ë³´ìƒ: {results['mean_reward']:.4f}")
                print(f"í‘œì¤€í¸ì°¨: {results['std_reward']:.4f}")
                print(f"í•™ìŠµ ì‹œê°„: {results['training_time']:.2f}ì´ˆ")
                print(f"ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {results['model_path']}")
                
                # ì„±ê³¼ ë“±ê¸‰ í‘œì‹œ
                if results['mean_reward'] > 0.8:
                    print("ğŸ¥‡ ìš°ìˆ˜í•œ ì„±ê³¼ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤!")
                elif results['mean_reward'] > 0.6:
                    print("ğŸ¥ˆ ì–‘í˜¸í•œ ì„±ê³¼ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤!")
                elif results['mean_reward'] > 0.4:
                    print("ğŸ¥‰ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
                else:
                    print("âš ï¸  ì¶”ê°€ í•™ìŠµì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        except KeyboardInterrupt:
            print("\ní”„ë¡œê·¸ë¨ì´ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()


if __name__ == "__main__":
    main() 