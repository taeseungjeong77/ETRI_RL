"""
Device utilities for automatic GPU/CPU selection
"""
import logging
import torch
import warnings
from typing import Optional, Tuple

# GPU κ΄€λ ¨ κ²½κ³  λ©”μ‹μ§€ μ–µμ 
warnings.filterwarnings("ignore", category=UserWarning, message=".*CUDA.*")


def check_gpu_availability() -> Tuple[bool, str]:
    """
    GPU μ‚¬μ© κ°€λ¥ μ—¬λ¶€λ¥Ό ν™•μΈν•©λ‹λ‹¤.
    
    Returns:
        Tuple[bool, str]: (GPU μ‚¬μ© κ°€λ¥ μ—¬λ¶€, λ””λ°”μ΄μ¤ μ •λ³΄)
    """
    try:
        if torch.cuda.is_available():
            gpu_count = torch.cuda.device_count()
            gpu_name = torch.cuda.get_device_name(0)
            return True, f"GPU μ‚¬μ© κ°€λ¥: {gpu_name} (μ΄ {gpu_count}κ°)"
        else:
            return False, "GPU μ‚¬μ© λ¶κ°€λ¥, CPU μ‚¬μ©"
    except Exception as e:
        logging.warning(f"GPU ν™•μΈ μ¤‘ μ¤λ¥ λ°μƒ: {e}")
        return False, "GPU ν™•μΈ μ‹¤ν¨, CPU μ‚¬μ©"


def get_device(force_cpu: bool = False) -> torch.device:
    """
    μµμ μ λ””λ°”μ΄μ¤λ¥Ό μλ™μΌλ΅ μ„ νƒν•©λ‹λ‹¤.
    
    Args:
        force_cpu (bool): CPU κ°•μ  μ‚¬μ© μ—¬λ¶€
        
    Returns:
        torch.device: μ„ νƒλ λ””λ°”μ΄μ¤
    """
    if force_cpu:
        device = torch.device("cpu")
        print("CPU κ°•μ  μ‚¬μ©")
        return device
    
    gpu_available, gpu_info = check_gpu_availability()
    print(f"λ””λ°”μ΄μ¤ μ •λ³΄: {gpu_info}")
    
    if gpu_available:
        device = torch.device("cuda")
        print("GPU ν™κ²½μ—μ„ μ‹¤ν–‰")
    else:
        device = torch.device("cpu")
        print("CPU ν™κ²½μ—μ„ μ‹¤ν–‰")
    
    return device


def setup_training_device(verbose: bool = True) -> dict:
    """
    ν•™μµ λ””λ°”μ΄μ¤ μ„¤μ • λ° μµμ ν™”λ ν•μ΄νΌνλΌλ―Έν„° λ°ν™ (κ°μ„ λ λ²„μ „)
    
    Returns:
        dict: λ””λ°”μ΄μ¤λ³„ μµμ ν™”λ ν•μ΄νΌνλΌλ―Έν„°
    """
    device = get_device()
    
    if device.type == "cuda":
        # GPU μ‚¬μ© μ‹ λ” μ κ·Ήμ μΈ ν•μ΄νΌνλΌλ―Έν„°
        config = {
            "learning_rate": 5e-4,  # λ” λ†’μ€ ν•™μµλ¥ 
            "n_steps": 2048,
            "batch_size": 512,      # λ” ν° λ°°μΉ ν¬κΈ°
            "n_epochs": 15,         # λ” λ§μ€ μ—ν¬ν¬
            "gamma": 0.995,         # λ†’μ€ κ°κ°€μ¨
            "gae_lambda": 0.95,
            "clip_range": 0.2,
            "ent_coef": 0.02,       # λ” λ†’μ€ μ—”νΈλ΅ν”Ό κ³„μ (νƒν— μ¥λ ¤)
            "vf_coef": 0.5,
            "max_grad_norm": 0.5,
            "device": device
        }
        if verbose:
            print(f"π€ GPU μµμ ν™” λ¨λ“ ν™μ„±ν™”: {torch.cuda.get_device_name()}")
            try:
                print(f"CUDA λ©”λ¨λ¦¬: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
            except:
                print("CUDA λ©”λ¨λ¦¬ μ •λ³΄λ¥Ό κ°€μ Έμ¬ μ μ—†μµλ‹λ‹¤")
            print("λ†’μ€ μ„±λ¥ ν•μ΄νΌνλΌλ―Έν„° μ μ©")
    else:
        # CPU μ‚¬μ© μ‹μ—λ„ κ°μ„ λ ν•μ΄νΌνλΌλ―Έν„°
        config = {
            "learning_rate": 3e-4,  # CPUμ—μ„λ„ λ” λ†’μ€ ν•™μµλ¥ 
            "n_steps": 1024,
            "batch_size": 256,      # λ” ν° λ°°μΉ ν¬κΈ°
            "n_epochs": 10,         # λ” λ§μ€ μ—ν¬ν¬
            "gamma": 0.99,
            "gae_lambda": 0.95,
            "clip_range": 0.2,
            "ent_coef": 0.01,
            "vf_coef": 0.5,
            "max_grad_norm": 0.5,
            "device": device
        }
        if verbose:
            print("π–¥οΈ  CPU μµμ ν™” λ¨λ“ (κ°μ„ λ ν•μ΄νΌνλΌλ―Έν„°)")
    
    if verbose:
        print(f"π“ κ°μ„ λ ν•μ΄νΌνλΌλ―Έν„°:")
        for key, value in config.items():
            if key != 'device':
                print(f"   - {key}: {value}")
    
    return config


def log_system_info():
    """μ‹μ¤ν… μ •λ³΄λ¥Ό λ΅κ·Έμ— κΈ°λ΅ν•©λ‹λ‹¤."""
    import platform
    import sys
    
    print("=== μ‹μ¤ν… μ •λ³΄ ===")
    print(f"ν”λ«νΌ: {platform.platform()}")
    print(f"Python λ²„μ „: {sys.version}")
    print(f"PyTorch λ²„μ „: {torch.__version__}")
    
    gpu_available, gpu_info = check_gpu_availability()
    print(f"GPU μ •λ³΄: {gpu_info}")
    
    if gpu_available:
        print(f"CUDA λ²„μ „: {torch.version.cuda}")
        print(f"cuDNN λ²„μ „: {torch.backends.cudnn.version()}")
        print(f"GPU λ©”λ¨λ¦¬:")
        for i in range(torch.cuda.device_count()):
            memory_total = torch.cuda.get_device_properties(i).total_memory / 1024**3
            print(f"  GPU {i}: {memory_total:.1f} GB")
    
    print("==================")


if __name__ == "__main__":
    log_system_info()
    device = get_device()
    config = setup_training_device()
    print(f"μ„ νƒλ λ””λ°”μ΄μ¤: {device}")
    print(f"ν•™μµ μ„¤μ •: {config}") 