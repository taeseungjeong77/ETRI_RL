#!/usr/bin/env python3
"""
Enhanced Optimization Script for 3D Bin Packing RL
Phase 4: ì •ë°€ ìµœì í™”ë¥¼ í†µí•œ ëª©í‘œ 18.57ì  ë‹¬ì„±

Based on Phase 3 results: Current best 16.116 -> Target 18.57 (+13.2% needed)
"""

import os
import sys
import json
import time
import numpy as np
import pandas as pd
from datetime import datetime
from typing import Dict, List, Tuple, Any, Optional
import warnings

# í™˜ê²½ ì„¤ì •
os.environ['CUDA_VISIBLE_DEVICES'] = ''
os.environ['MPLBACKEND'] = 'Agg'
warnings.filterwarnings("ignore")

# src í´ë”ë¥¼ pathì— ì¶”ê°€
sys.path.append('src')

import gymnasium as gym
from gymnasium.envs.registration import register
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor
from stable_baselines3.common.callbacks import BaseCallback
from sb3_contrib import MaskablePPO
from sb3_contrib.common.wrappers import ActionMasker
import torch
import matplotlib.pyplot as plt
import seaborn as sns

try:
    from packing_env import PackingEnv
    from train_maskable_ppo import ImprovedRewardWrapper
    from utils import boxes_generator
    print("âœ… ëª¨ë“  ëª¨ë“ˆ import ì„±ê³µ")
except ImportError as e:
    print(f"âŒ Import ì˜¤ë¥˜: {e}")
    print("src í´ë”ì™€ í•„ìš”í•œ ëª¨ë“ˆë“¤ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
    sys.exit(1)

def get_env_info(env):
    """í™˜ê²½ ì •ë³´ë¥¼ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜"""
    try:
        # ë˜í¼ë“¤ì„ ë²—ê²¨ë‚´ì–´ ì‹¤ì œ PackingEnvì— ì ‘ê·¼
        unwrapped_env = env
        while hasattr(unwrapped_env, 'env'):
            unwrapped_env = unwrapped_env.env
        
        # ì»¨í…Œì´ë„ˆ í¬ê¸°
        if hasattr(unwrapped_env, 'container') and hasattr(unwrapped_env.container, 'size'):
            container_size = unwrapped_env.container.size
        else:
            container_size = [10, 10, 10]  # ê¸°ë³¸ê°’
        
        # ë°•ìŠ¤ ê°œìˆ˜
        if hasattr(unwrapped_env, 'initial_boxes'):
            box_count = len(unwrapped_env.initial_boxes)
        elif hasattr(unwrapped_env, 'num_initial_boxes'):
            box_count = unwrapped_env.num_initial_boxes
        else:
            box_count = 12  # ê¸°ë³¸ê°’
        
        return container_size, box_count
    except Exception as e:
        print(f"âš ï¸ í™˜ê²½ ì •ë³´ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        return [10, 10, 10], 12

def calculate_utilization_and_items(env):
    """í™œìš©ë¥ ê³¼ ë°°ì¹˜ëœ ë°•ìŠ¤ ê°œìˆ˜ ê³„ì‚° (ê¸°ì¡´ í”„ë¡œì íŠ¸ ë°©ì‹)"""
    try:
        # ë˜í¼ë“¤ì„ ë²—ê²¨ë‚´ì–´ ì‹¤ì œ PackingEnvì— ì ‘ê·¼
        unwrapped_env = env
        while hasattr(unwrapped_env, 'env'):
            unwrapped_env = unwrapped_env.env
        
        if hasattr(unwrapped_env, 'container'):
            # ë°°ì¹˜ëœ ë°•ìŠ¤ë“¤ì˜ ë³¼ë¥¨ ê³„ì‚°
            placed_volume = 0
            placed_count = 0
            
            for box in unwrapped_env.container.boxes:
                if hasattr(box, 'position') and box.position is not None:
                    # positionì´ [-1, -1, -1]ì´ ì•„ë‹Œ ê²½ìš°ë§Œ ë°°ì¹˜ëœ ê²ƒìœ¼ë¡œ ê°„ì£¼
                    if not (box.position[0] == -1 and box.position[1] == -1 and box.position[2] == -1):
                        placed_volume += box.volume
                        placed_count += 1
            
            # ì»¨í…Œì´ë„ˆ ì „ì²´ ë³¼ë¥¨
            container_volume = unwrapped_env.container.volume
            utilization = placed_volume / container_volume if container_volume > 0 else 0.0
            
            return utilization, placed_count
        else:
            return 0.0, 0
    except Exception as e:
        print(f"âš ï¸ í™œìš©ë¥  ê³„ì‚° ì‹¤íŒ¨: {e}")
        return 0.0, 0

class EnhancedOptimizer:
    """Phase 4: ì •ë°€ ìµœì í™” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.results_dir = "results"
        os.makedirs(self.results_dir, exist_ok=True)
        
        # Phase 3 ìµœê³  ì„±ëŠ¥ ê¸°ì¤€ì 
        self.phase3_best = {
            'score': 16.116,
            'params': {
                'learning_rate': 0.00015,
                'n_steps': 512,
                'batch_size': 128,
                'n_epochs': 4,
                'clip_range': 0.2,
                'ent_coef': 0.01,
                'vf_coef': 0.5,
                'gae_lambda': 0.95,
                'net_arch': [256, 128, 64]
            }
        }
        
        self.target_score = 18.57
        self.improvement_needed = (self.target_score - self.phase3_best['score']) / self.phase3_best['score']
        
        print(f"ğŸ¯ Phase 4 Enhanced Optimization ì‹œì‘")
        print(f"ğŸ“Š ê¸°ì¤€ì : {self.phase3_best['score']:.3f}ì ")
        print(f"ğŸ† ëª©í‘œ: {self.target_score}ì  ({self.improvement_needed:.1%} ê°œì„  í•„ìš”)")
        
    def create_enhanced_environment(self, num_boxes: int = 12, container_size: List[int] = [10, 10, 10], 
                                  enhanced_reward: bool = True, seed: int = 42) -> gym.Env:
        """í–¥ìƒëœ í™˜ê²½ ìƒì„± (ê¸°ì¡´ í”„ë¡œì íŠ¸ ë°©ì‹ ì ìš©)"""
        try:
            print(f"ìƒì„±ëœ ë°•ìŠ¤ ê°œìˆ˜: {num_boxes}")
            print(f"ì»¨í…Œì´ë„ˆ í¬ê¸°: {container_size}")
            
            # PackingEnv ë“±ë¡ (ì´ë¯¸ ë“±ë¡ë˜ì–´ ìˆì§€ ì•Šì€ ê²½ìš°)
            if 'PackingEnv-v0' not in gym.envs.registry:
                register(id='PackingEnv-v0', entry_point='packing_env:PackingEnv')
            
            # ë°•ìŠ¤ ìƒì„±
            box_sizes = boxes_generator(container_size, num_boxes, seed)
            
            # í™˜ê²½ ìƒì„±
            env = gym.make(
                "PackingEnv-v0",
                container_size=container_size,
                box_sizes=box_sizes,
                num_visible_boxes=min(3, num_boxes),
                render_mode=None,
                random_boxes=False,
                only_terminal_reward=False,
            )
            print("í™˜ê²½ ìƒì„± ì„±ê³µ: PackingEnv-v0")
            
            # ë³´ìƒ ë˜í¼ ì ìš©
            if enhanced_reward:
                env = EnhancedRewardWrapper(env)
                print("ê°•í™”ëœ ë³´ìƒ ë˜í¼ ì ìš©ë¨")
            else:
                env = ImprovedRewardWrapper(env)
                print("ê°œì„ ëœ ë³´ìƒ ë˜í¼ ì ìš©ë¨")
                
            # Action Masker ì ìš© (ê¸°ì¡´ í”„ë¡œì íŠ¸ ë°©ì‹)
            def get_action_masks(env):
                """ì•¡ì…˜ ë§ˆìŠ¤í¬ ìƒì„±"""
                try:
                    # ë˜í¼ë“¤ì„ ë²—ê²¨ë‚´ì–´ ì‹¤ì œ PackingEnvì— ì ‘ê·¼
                    unwrapped_env = env
                    while hasattr(unwrapped_env, 'env'):
                        unwrapped_env = unwrapped_env.env
                    
                    if hasattr(unwrapped_env, 'action_masks'):
                        masks = unwrapped_env.action_masks()
                        if isinstance(masks, list):
                            return np.array(masks, dtype=bool)
                        return masks
                    else:
                        # ê¸°ë³¸ì ìœ¼ë¡œ ëª¨ë“  ì•¡ì…˜ í—ˆìš©
                        return np.ones(env.action_space.n, dtype=bool)
                except Exception as e:
                    print(f"âš ï¸ ì•¡ì…˜ ë§ˆìŠ¤í¬ ìƒì„± ì‹¤íŒ¨: {e}")
                    return np.ones(env.action_space.n, dtype=bool)
            
            env = ActionMasker(env, get_action_masks)
            print("ì•¡ì…˜ ë§ˆìŠ¤í‚¹ ë˜í¼ ì ìš©ë¨")
            
            print(f"ì‹œë“œ ì„¤ì • ì™„ë£Œ: {seed}")
            
            return env
            
        except Exception as e:
            print(f"âŒ í™˜ê²½ ìƒì„± ì‹¤íŒ¨: {e}")
            raise e
    
    def get_enhanced_parameter_sets(self) -> Dict[str, Dict]:
        """ê°•í™”ëœ íŒŒë¼ë¯¸í„° ì„¸íŠ¸ë“¤"""
        
        # 1. í•™ìŠµ ì•ˆì •ì„± ê°•í™” ì„¸íŠ¸
        stability_sets = {
            'stability_conservative': {
                'learning_rate': 1.2e-04,  # ë” ë³´ìˆ˜ì 
                'n_steps': 1024,           # ë” ë§ì€ ê²½í—˜
                'batch_size': 64,          # ì•ˆì •ì  ì—…ë°ì´íŠ¸
                'n_epochs': 6,             # ë” ê¹Šì€ í•™ìŠµ
                'clip_range': 0.15,        # ë³´ìˆ˜ì  ì—…ë°ì´íŠ¸
                'ent_coef': 0.005,         # íƒìƒ‰ ê°ì†Œ
                'vf_coef': 0.5,
                'gae_lambda': 0.98,        # ì¥ê¸° ë³´ìƒ ì¤‘ì‹œ
                'net_arch': [dict(pi=[256, 128, 64], vf=[256, 128, 64])]
            },
            'stability_balanced': {
                'learning_rate': 1.3e-04,
                'n_steps': 768,
                'batch_size': 96,
                'n_epochs': 5,
                'clip_range': 0.18,
                'ent_coef': 0.008,
                'vf_coef': 0.5,
                'gae_lambda': 0.96,
                'net_arch': [dict(pi=[256, 128, 64], vf=[256, 128, 64])]
            }
        }
        
        # 2. ê³ ê¸‰ ë„¤íŠ¸ì›Œí¬ ì•„í‚¤í…ì²˜ ì„¸íŠ¸
        architecture_sets = {
            'arch_wide': {
                'learning_rate': 1.5e-04,
                'n_steps': 512,
                'batch_size': 128,
                'n_epochs': 4,
                'clip_range': 0.2,
                'ent_coef': 0.01,
                'vf_coef': 0.5,
                'gae_lambda': 0.95,
                'net_arch': [dict(pi=[512, 256, 128], vf=[512, 256, 128])]  # ë” í° ì²« ë ˆì´ì–´
            },
            'arch_deep': {
                'learning_rate': 1.4e-04,
                'n_steps': 512,
                'batch_size': 128,
                'n_epochs': 4,
                'clip_range': 0.2,
                'ent_coef': 0.01,
                'vf_coef': 0.5,
                'gae_lambda': 0.95,
                'net_arch': [dict(pi=[256, 256, 128, 64], vf=[256, 256, 128, 64])]  # ì¶”ê°€ ë ˆì´ì–´
            },
            'arch_balanced': {
                'learning_rate': 1.5e-04,
                'n_steps': 512,
                'batch_size': 128,
                'n_epochs': 4,
                'clip_range': 0.2,
                'ent_coef': 0.01,
                'vf_coef': 0.5,
                'gae_lambda': 0.95,
                'net_arch': [dict(pi=[384, 192, 96], vf=[384, 192, 96])]  # ê· í˜• ì¡íŒ ê°ì†Œ
            },
            'arch_reinforced': {
                'learning_rate': 1.5e-04,
                'n_steps': 512,
                'batch_size': 128,
                'n_epochs': 4,
                'clip_range': 0.2,
                'ent_coef': 0.01,
                'vf_coef': 0.5,
                'gae_lambda': 0.95,
                'net_arch': [dict(pi=[256, 128, 128, 64], vf=[256, 128, 128, 64])]  # ì¤‘ê°„ ë ˆì´ì–´ ê°•í™”
            }
        }
        
        # 3. ìµœì í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¸íŠ¸
        optimized_sets = {
            'opt_precision': {
                'learning_rate': 1.1e-04,  # ì •ë°€ í•™ìŠµ
                'n_steps': 1536,           # ë§ì€ ê²½í—˜
                'batch_size': 192,         # í° ë°°ì¹˜
                'n_epochs': 8,             # ê¹Šì€ í•™ìŠµ
                'clip_range': 0.12,        # ë§¤ìš° ë³´ìˆ˜ì 
                'ent_coef': 0.003,         # ìµœì†Œ íƒìƒ‰
                'vf_coef': 0.6,            # ê°€ì¹˜ í•¨ìˆ˜ ì¤‘ì‹œ
                'gae_lambda': 0.99,        # ìµœëŒ€ ì¥ê¸° ë³´ìƒ
                'net_arch': [dict(pi=[256, 128, 64], vf=[256, 128, 64])]
            },
            'opt_aggressive': {
                'learning_rate': 1.8e-04,  # ì ê·¹ì  í•™ìŠµ
                'n_steps': 256,            # ë¹ ë¥¸ ì—…ë°ì´íŠ¸
                'batch_size': 64,          # ì‘ì€ ë°°ì¹˜
                'n_epochs': 3,             # ë¹ ë¥¸ ì—í¬í¬
                'clip_range': 0.25,        # í° ì—…ë°ì´íŠ¸
                'ent_coef': 0.02,          # ë§ì€ íƒìƒ‰
                'vf_coef': 0.4,
                'gae_lambda': 0.92,
                'net_arch': [dict(pi=[256, 128, 64], vf=[256, 128, 64])]
            }
        }
        
        # ëª¨ë“  ì„¸íŠ¸ ê²°í•©
        all_sets = {}
        all_sets.update(stability_sets)
        all_sets.update(architecture_sets)
        all_sets.update(optimized_sets)
        
        return all_sets
    
    def train_and_evaluate(self, params: Dict, name: str, timesteps: int = 35000, 
                          eval_episodes: int = 25, enhanced_reward: bool = True) -> Dict[str, Any]:
        """ëª¨ë¸ í›ˆë ¨ ë° í‰ê°€ (ê¸°ì¡´ í”„ë¡œì íŠ¸ ë°©ì‹ ì ìš©)"""
        print(f"\nğŸ”§ {name} ìµœì í™” ì¤‘...")
        
        # í™˜ê²½ ìƒì„± (í›ˆë ¨ìš©)
        env = self.create_enhanced_environment(enhanced_reward=enhanced_reward, seed=42)
        container_size, box_count = get_env_info(env)
        print(f"âœ… í™˜ê²½ ìƒì„± ì„±ê³µ: ì»¨í…Œì´ë„ˆ{container_size}, ë°•ìŠ¤{box_count}ê°œ")
        
        # ëª¨ë¸ ìƒì„± - MaskablePPO ì‚¬ìš©
        model = MaskablePPO(
            'MultiInputPolicy',
            env,
            learning_rate=params['learning_rate'],
            n_steps=params['n_steps'],
            batch_size=params['batch_size'],
            n_epochs=params['n_epochs'],
            clip_range=params['clip_range'],
            ent_coef=params['ent_coef'],
            vf_coef=params['vf_coef'],
            gae_lambda=params['gae_lambda'],
            policy_kwargs={'net_arch': params['net_arch']},
            verbose=0,
            device='auto'
        )
        
        # í›ˆë ¨
        print(f"ğŸ“ {name} í•™ìŠµ ì‹œì‘: {timesteps:,} ìŠ¤í… (LR: {params['learning_rate']:.2e}, Net: {params['net_arch']})")
        start_time = time.time()
        
        model.learn(total_timesteps=timesteps)
        
        training_time = time.time() - start_time
        print(f"â±ï¸ {name} í•™ìŠµ ì™„ë£Œ: {training_time:.1f}ì´ˆ")
        
        # í‰ê°€ (ê¸°ì¡´ í”„ë¡œì íŠ¸ ë°©ì‹)
        print(f"ğŸ” {name} í‰ê°€ ì‹œì‘ ({eval_episodes} ì—í”¼ì†Œë“œ, ìµœëŒ€ 25 ìŠ¤í…)")
        
        rewards = []
        utilizations = []
        placements = []
        
        for i in range(eval_episodes):
            eval_env = self.create_enhanced_environment(enhanced_reward=enhanced_reward, seed=100 + i * 5)
            container_size, box_count = get_env_info(eval_env)
            
            # í™˜ê²½ ë¦¬ì…‹ (seed í¬í•¨)
            obs = eval_env.reset(seed=100 + i * 5)
            if isinstance(obs, tuple):
                obs = obs[0]
                
            episode_reward = 0
            step_count = 0
            max_steps = 25  # ê¸°ì¡´ í”„ë¡œì íŠ¸ì™€ ë™ì¼
            
            while step_count < max_steps:
                try:
                    # ê¸°ì¡´ í”„ë¡œì íŠ¸ ë°©ì‹: deterministic=False, action_masks ì‚¬ìš© ì•ˆ í•¨
                    action, _ = model.predict(obs, deterministic=False)
                    obs, reward, terminated, truncated, info = eval_env.step(action)
                    episode_reward += reward
                    step_count += 1
                    
                    if terminated or truncated:
                        break
                except Exception as e:
                    print(f"âš ï¸ í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
                    break
            
            # í™œìš©ë¥ ê³¼ ë°°ì¹˜ ê°œìˆ˜ ê³„ì‚° (ê¸°ì¡´ í”„ë¡œì íŠ¸ ë°©ì‹)
            final_utilization, placement_count = calculate_utilization_and_items(eval_env)
            
            rewards.append(episode_reward)
            utilizations.append(final_utilization)
            placements.append(placement_count)
            
            # ì£¼ìš” ì—í”¼ì†Œë“œë§Œ ì¶œë ¥
            if i < 6 or i in [10, 15, 20] or i == eval_episodes - 1:
                print(f"   ì—í”¼ì†Œë“œ {i+1}: ë³´ìƒ={episode_reward:.3f}, í™œìš©ë¥ ={final_utilization:.1%}, ë°•ìŠ¤={placement_count}ê°œ")
            
            eval_env.close()
        
        # ê²°ê³¼ ê³„ì‚°
        mean_reward = np.mean(rewards)
        std_reward = np.std(rewards)
        mean_utilization = np.mean(utilizations)
        std_utilization = np.std(utilizations)
        mean_placement = np.mean(placements)
        max_placement = np.max(placements)
        
        # ì„±ê³µë¥  (5ê°œ ì´ìƒ ë°°ì¹˜)
        success_count = sum(1 for p in placements if p >= 5)
        success_rate = success_count / eval_episodes
        
        # ì¢…í•© ì ìˆ˜ ê³„ì‚° (ê¸°ì¡´ í”„ë¡œì íŠ¸ ë°©ì‹ ì ìš©)
        combined_score = mean_reward * 0.3 + mean_utilization * 100 * 0.7
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"ğŸ“Š {name} ìµœì¢… ê²°ê³¼:")
        print(f"   í‰ê·  ë³´ìƒ: {mean_reward:.3f} Â± {std_reward:.3f}")
        print(f"   í‰ê·  í™œìš©ë¥ : {mean_utilization:.1%} Â± {std_utilization:.1%}")
        print(f"   í‰ê·  ë°°ì¹˜: {mean_placement:.1f}ê°œ (ìµœëŒ€: {max_placement}ê°œ)")
        print(f"   ì„±ê³µë¥ : {success_rate:.1%}")
        print(f"   ì¢…í•© ì ìˆ˜: {combined_score:.3f}")
        
        # í™˜ê²½ ì •ë¦¬
        env.close()
        
        return {
            'mean_reward': mean_reward,
            'std_reward': std_reward,
            'mean_utilization': mean_utilization,
            'std_utilization': std_utilization,
            'mean_placement': mean_placement,
            'max_placement': max_placement,
            'success_rate': success_rate,
            'combined_score': combined_score,
            'episodes': eval_episodes,
            'training_time': training_time,
            'params': params
        }
    
    def run_phase4_optimization(self, focus: str = 'all', timesteps: int = 35000) -> Dict:
        """Phase 4 ìµœì í™” ì‹¤í–‰"""
        print(f"\n{'='*60}")
        print(f"ğŸš€ Phase 4 Enhanced Optimization ì‹œì‘")
        print(f"ğŸ¯ í¬ì»¤ìŠ¤: {focus}")
        print(f"â±ï¸ í•™ìŠµ ìŠ¤í…: {timesteps:,}")
        print(f"{'='*60}")
        
        all_params = self.get_enhanced_parameter_sets()
        results = {}
        best_score = 0
        best_config = None
        
        # í¬ì»¤ìŠ¤ì— ë”°ë¥¸ í•„í„°ë§
        if focus == 'stability':
            params_to_test = {k: v for k, v in all_params.items() if k.startswith('stability')}
        elif focus == 'architecture':
            params_to_test = {k: v for k, v in all_params.items() if k.startswith('arch')}
        elif focus == 'optimization':
            params_to_test = {k: v for k, v in all_params.items() if k.startswith('opt')}
        else:
            params_to_test = all_params
        
        print(f"ğŸ“‹ í…ŒìŠ¤íŠ¸í•  ì„¤ì •: {len(params_to_test)}ê°œ")
        
        total_start_time = time.time()
        
        for i, (name, params) in enumerate(params_to_test.items(), 1):
            print(f"\n[{i}/{len(params_to_test)}] {name} í…ŒìŠ¤íŠ¸ ì¤‘...")
            
            try:
                result = self.train_and_evaluate(
                    params, name, timesteps=timesteps, 
                    enhanced_reward=True  # ê°•í™”ëœ ë³´ìƒ ì‚¬ìš©
                )
                results[name] = result
                
                # ìµœê³  ì„±ëŠ¥ ì—…ë°ì´íŠ¸
                if result['combined_score'] > best_score:
                    best_score = result['combined_score']
                    best_config = name
                    print(f"ğŸ† ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥: {best_score:.3f}ì ")
                
            except Exception as e:
                print(f"âŒ {name} ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        total_time = time.time() - total_start_time
        
        # ê²°ê³¼ ì •ë¦¬ ë° ì¶œë ¥
        if results:
            print(f"\n{'='*60}")
            print(f"ğŸ† Phase 4 ìµœì í™” ê²°ê³¼")
            print(f"{'='*60}")
            
            # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
            sorted_results = sorted(results.items(), key=lambda x: x[1]['combined_score'], reverse=True)
            
            print("ìˆœìœ„  ì„¤ì •ëª…                    ì ìˆ˜      ê°œì„ ìœ¨   í™œìš©ë¥    ì„±ê³µë¥ ")
            print("-" * 70)
            
            for rank, (name, result) in enumerate(sorted_results[:10], 1):
                improvement = (result['combined_score'] - self.phase3_best['score']) / self.phase3_best['score'] * 100
                print(f"{rank:2d}    {name:<22} {result['combined_score']:6.2f}   {improvement:+5.1f}%   "
                      f"{result['mean_utilization']:5.1%}   {result['success_rate']:5.1%}")
            
            best_result = sorted_results[0][1]
            target_achievement = best_score / self.target_score * 100
            
            print(f"\nğŸ† ìµœê³  ì„±ëŠ¥: {best_score:.3f}ì  ({best_config})")
            print(f"ğŸ“ˆ ëª©í‘œ ë‹¬ì„±ë„: {target_achievement:.1f}% (ëª©í‘œ {self.target_score} ëŒ€ë¹„)")
            
            if best_score >= self.target_score:
                print(f"ğŸ‰ ëª©í‘œ ë‹¬ì„± ì„±ê³µ!")
            else:
                remaining = self.target_score - best_score
                print(f"ğŸ“Š ëª©í‘œê¹Œì§€ {remaining:.3f}ì  ë¶€ì¡±")
            
            # NumPy íƒ€ì…ì„ Python ê¸°ë³¸ íƒ€ì…ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
            def convert_numpy_types(obj):
                """NumPy íƒ€ì…ì„ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ íƒ€ì…ìœ¼ë¡œ ë³€í™˜"""
                if isinstance(obj, np.integer):
                    return int(obj)
                elif isinstance(obj, np.floating):
                    return float(obj)
                elif isinstance(obj, np.ndarray):
                    return obj.tolist()
                elif isinstance(obj, dict):
                    return {key: convert_numpy_types(value) for key, value in obj.items()}
                elif isinstance(obj, list):
                    return [convert_numpy_types(item) for item in obj]
                else:
                    return obj
            
            # ê²°ê³¼ë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
            converted_results = convert_numpy_types(results)
            
            # ê²°ê³¼ ì €ì¥
            output_data = {
                'timestamp': self.timestamp,
                'phase': 'phase4_enhanced_optimization',
                'focus': focus,
                'timesteps': int(timesteps),  # ëª…ì‹œì ìœ¼ë¡œ int ë³€í™˜
                'target_score': float(self.target_score),  # ëª…ì‹œì ìœ¼ë¡œ float ë³€í™˜
                'phase3_baseline': float(self.phase3_best['score']),
                'best_score': float(best_score),
                'best_config': best_config,
                'target_achievement': float(target_achievement),
                'total_time_minutes': float(total_time / 60),
                'results': converted_results
            }
            
            output_file = os.path.join(self.results_dir, f'phase4_enhanced_{focus}_{self.timestamp}.json')
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, indent=2, ensure_ascii=False)
            
            print(f"ğŸ’¾ Phase 4 ê²°ê³¼: {output_file}")
            print(f"â±ï¸ ì´ ì†Œìš” ì‹œê°„: {total_time/60:.1f}ë¶„")
            
            return output_data
        
        else:
            print("âŒ ìœ íš¨í•œ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {}
    
    def create_performance_analysis(self, results_file: str):
        """ì„±ëŠ¥ ë¶„ì„ ì°¨íŠ¸ ìƒì„±"""
        try:
            with open(results_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            if not data.get('results'):
                print("ë¶„ì„í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            # ë°ì´í„° ì¤€ë¹„
            configs = []
            scores = []
            utilizations = []
            success_rates = []
            
            for name, result in data['results'].items():
                configs.append(name)
                scores.append(result['combined_score'])
                utilizations.append(result['mean_utilization'] * 100)
                success_rates.append(result['success_rate'] * 100)
            
            # ì°¨íŠ¸ ìƒì„±
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            fig.suptitle(f'Phase 4 Enhanced Optimization Analysis\n'
                        f'Best: {data["best_score"]:.2f} (Target: {data["target_score"]})', 
                        fontsize=16, fontweight='bold')
            
            # 1. ì¢…í•© ì ìˆ˜
            axes[0,0].bar(range(len(configs)), scores, color='skyblue', alpha=0.7)
            axes[0,0].axhline(y=data['target_score'], color='red', linestyle='--', label=f'Target: {data["target_score"]}')
            axes[0,0].axhline(y=data['phase3_baseline'], color='orange', linestyle='--', label=f'Phase3: {data["phase3_baseline"]:.2f}')
            axes[0,0].set_title('Combined Scores')
            axes[0,0].set_ylabel('Score')
            axes[0,0].legend()
            axes[0,0].tick_params(axis='x', rotation=45)
            
            # 2. í™œìš©ë¥ 
            axes[0,1].bar(range(len(configs)), utilizations, color='lightgreen', alpha=0.7)
            axes[0,1].set_title('Space Utilization (%)')
            axes[0,1].set_ylabel('Utilization %')
            axes[0,1].tick_params(axis='x', rotation=45)
            
            # 3. ì„±ê³µë¥ 
            axes[1,0].bar(range(len(configs)), success_rates, color='lightcoral', alpha=0.7)
            axes[1,0].set_title('Success Rate (%)')
            axes[1,0].set_ylabel('Success Rate %')
            axes[1,0].tick_params(axis='x', rotation=45)
            
            # 4. ìƒê´€ê´€ê³„
            axes[1,1].scatter(utilizations, scores, alpha=0.7, s=100)
            axes[1,1].set_xlabel('Utilization %')
            axes[1,1].set_ylabel('Combined Score')
            axes[1,1].set_title('Utilization vs Score')
            
            # xì¶• ë¼ë²¨ ì„¤ì •
            for ax in axes.flat:
                if hasattr(ax, 'set_xticks'):
                    ax.set_xticks(range(len(configs)))
                    ax.set_xticklabels([c[:15] + '...' if len(c) > 15 else c for c in configs], 
                                     rotation=45, ha='right')
            
            plt.tight_layout()
            
            # ì €ì¥
            chart_file = results_file.replace('.json', '_analysis.png')
            plt.savefig(chart_file, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š ë¶„ì„ ì°¨íŠ¸ ì €ì¥: {chart_file}")
            
            plt.close()
            
        except Exception as e:
            print(f"âŒ ì°¨íŠ¸ ìƒì„± ì˜¤ë¥˜: {e}")


class EnhancedRewardWrapper(gym.RewardWrapper):
    """ê°•í™”ëœ ë³´ìƒ ë˜í¼ - í™œìš©ë¥ ê³¼ ë°°ì¹˜ íš¨ìœ¨ì„± ê·¹ëŒ€í™”"""
    
    def __init__(self, env):
        super().__init__(env)
        self.previous_utilization = 0.0
        self.consecutive_placements = 0
        
    def reset(self, **kwargs):
        self.previous_utilization = 0.0
        self.consecutive_placements = 0
        return self.env.reset(**kwargs)
    
    def step(self, action):
        obs, reward, terminated, truncated, info = self.env.step(action)
        
        # ê°•í™”ëœ ë³´ìƒ ê³„ì‚°
        enhanced_reward = self.reward(reward)
        
        return obs, enhanced_reward, terminated, truncated, info
        
    def reward(self, reward):
        # í˜„ì¬ í™œìš©ë¥ ê³¼ ë°°ì¹˜ ê°œìˆ˜ ê³„ì‚°
        current_utilization, placement_count = calculate_utilization_and_items(self.env)
        
        # 1. ê¸°ë³¸ ë³´ìƒ
        enhanced_reward = reward
        
        # 2. í™œìš©ë¥  ê°œì„  ë³´ìƒ (ì œê³±ê·¼ -> 1.5ìŠ¹ìœ¼ë¡œ ê°•í™”)
        if current_utilization > 0:
            util_bonus = (current_utilization ** 1.5) * 3.0
            enhanced_reward += util_bonus
        
        # 3. í™œìš©ë¥  ì¦ê°€ ë³´ìƒ
        if current_utilization > self.previous_utilization:
            improvement_bonus = (current_utilization - self.previous_utilization) * 5.0
            enhanced_reward += improvement_bonus
        
        # 4. ì—°ì† ë°°ì¹˜ ë³´ë„ˆìŠ¤
        if placement_count > 0:
            self.consecutive_placements += 1
            consecutive_bonus = min(self.consecutive_placements * 0.1, 1.0)
            enhanced_reward += consecutive_bonus
        else:
            self.consecutive_placements = 0
        
        # 5. ì„ê³„ê°’ ëŒíŒŒ ë³´ë„ˆìŠ¤
        if current_utilization > 0.25:
            threshold_bonus = 2.0
            enhanced_reward += threshold_bonus
        elif current_utilization > 0.20:
            threshold_bonus = 1.0
            enhanced_reward += threshold_bonus
        
        # 6. ë†’ì€ ë°°ì¹˜ ìˆ˜ ë³´ë„ˆìŠ¤
        if placement_count >= 5:
            placement_bonus = (placement_count - 4) * 0.5
            enhanced_reward += placement_bonus
        
        # 7. ê³µê°„ íš¨ìœ¨ì„± ë³´ë„ˆìŠ¤ (ë°•ìŠ¤ë‹¹ í™œìš©ë¥ )
        if placement_count > 0:
            efficiency = current_utilization / placement_count
            efficiency_bonus = efficiency * 2.0
            enhanced_reward += efficiency_bonus
        
        self.previous_utilization = current_utilization
        
        return enhanced_reward


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Enhanced Optimization for 3D Bin Packing')
    parser.add_argument('--focus', choices=['all', 'stability', 'architecture', 'optimization'], 
                       default='all', help='Optimization focus area')
    parser.add_argument('--timesteps', type=int, default=35000, help='Training timesteps')
    parser.add_argument('--analyze', type=str, help='Analyze results from JSON file')
    
    args = parser.parse_args()
    
    optimizer = EnhancedOptimizer()
    
    if args.analyze:
        # ê²°ê³¼ ë¶„ì„ ëª¨ë“œ
        optimizer.create_performance_analysis(args.analyze)
    else:
        # ìµœì í™” ì‹¤í–‰ ëª¨ë“œ
        result = optimizer.run_phase4_optimization(focus=args.focus, timesteps=args.timesteps)
        
        if result and result.get('results'):
            # ìë™ìœ¼ë¡œ ë¶„ì„ ì°¨íŠ¸ ìƒì„±
            output_file = os.path.join(optimizer.results_dir, 
                                     f'phase4_enhanced_{args.focus}_{optimizer.timestamp}.json')
            optimizer.create_performance_analysis(output_file)
            
            # ë‹¤ìŒ ë‹¨ê³„ ê¶Œì¥ì‚¬í•­
            best_score = result['best_score']
            target_score = result['target_score']
            
            if best_score >= target_score:
                print(f"\nğŸ‰ ì¶•í•˜í•©ë‹ˆë‹¤! ëª©í‘œ {target_score}ì ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤!")
                print(f"ğŸ† ìµœì¢… ì„±ëŠ¥: {best_score:.3f}ì ")
            else:
                remaining = target_score - best_score
                print(f"\nğŸ“Š ì¶”ê°€ ê°œì„  ê¶Œì¥ì‚¬í•­:")
                print(f"   ëª©í‘œê¹Œì§€ {remaining:.3f}ì  ë¶€ì¡±")
                
                if remaining > 1.0:
                    print(f"   â¡ï¸ í•™ìŠµ ì‹œê°„ì„ 50,000ìŠ¤í…ìœ¼ë¡œ ì¦ê°€ ì‹œë„")
                    print(f"   â¡ï¸ ì•™ìƒë¸” ëª¨ë¸ë§ ì‹œë„")
                else:
                    print(f"   â¡ï¸ ë¯¸ì„¸ ì¡°ì •ìœ¼ë¡œ ë‹¬ì„± ê°€ëŠ¥")
                    print(f"   â¡ï¸ ë³´ìƒ í•¨ìˆ˜ ì¶”ê°€ ìµœì í™” ê¶Œì¥")


if __name__ == "__main__":
    main() 